"""
AI Analysis Routes - An√°lisis optimizado con Base64 (SIN GCS, SIN NGROK)
Endpoints para an√°lisis con OpenAI usando codificaci√≥n Base64 en memoria
"""
import threading
import socketserver
import http.server
import json
import re
from json_repair import repair_json
from fastapi import APIRouter, HTTPException, UploadFile, File
from typing import Dict, Any
import os
from pyngrok import ngrok
from datetime import datetime
import logging
from pathlib import Path

from ..utils.config import get_facebook_saved_base

# Configurar logger
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

router = APIRouter(tags=["Facebook"])


def start_local_file_server(directory, port=8000):
    """Inicia servidor HTTP local para servir archivos"""
    handler = http.server.SimpleHTTPRequestHandler
    os.chdir(directory)
    httpd = socketserver.TCPServer(("", port), handler)
    thread = threading.Thread(target=httpd.serve_forever, daemon=True)
    thread.start()
    return httpd, thread


@router.post(
    '/expose-single-file',
    tags=["ai-analysis"],
    summary="Expone archivo y analiza con OpenAI (ngrok + prompt personalizado)"
)
async def expose_single_file(
    file: UploadFile = File(...),
    prompt: str = "Analiza esta imagen publicitaria y describe todos los elementos visuales, texto, y estrategia de marketing que observes."
) -> Dict[str, Any]:
    """
    Expone un archivo con URL p√∫blica via ngrok y lo analiza con OpenAI Vision.

    Este endpoint:
    1. Guarda el archivo subido temporalmente
    2. Inicia un servidor HTTP local
    3. Crea un t√∫nel ngrok para URL p√∫blica
    4. Env√≠a la URL a OpenAI Vision con el prompt
    5. Retorna an√°lisis completo + URL p√∫blica

    Args:
        file: Archivo a analizar (imagen/video - selector de archivo)
        prompt: Prompt personalizado para el an√°lisis

    Returns:
        JSON con an√°lisis de OpenAI y URL p√∫blica del archivo
    """
    filename = file.filename
    logger.info(f"üîç Analizando archivo: {filename}")

    try:
        # Crear directorio temporal
        temp_dir = Path("temp_ngrok_files") / "uploads"
        temp_dir.mkdir(parents=True, exist_ok=True)

        # Guardar archivo subido
        file_path = temp_dir / filename

        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)

        file_size = file_path.stat().st_size
        logger.info(f"   ‚úÖ Archivo guardado: {file_path} ({file_size} bytes)")

        # Iniciar servidor HTTP en puerto libre
        import random
        port = random.randint(8100, 8999)

        class QuietHTTPHandler(http.server.SimpleHTTPRequestHandler):
            def log_message(self, format, *args):
                pass  # Silenciar logs del servidor

        os.chdir(temp_dir)
        httpd = socketserver.TCPServer(("", port), QuietHTTPHandler)
        thread = threading.Thread(target=httpd.serve_forever, daemon=True)
        thread.start()
        logger.info(f"   üåê Servidor HTTP iniciado en puerto {port}")

        # Crear t√∫nel ngrok
        tunnel = ngrok.connect(port)
        public_url = tunnel.public_url
        file_url = f"{public_url}/{filename}"

        logger.info(f"   ‚úÖ T√∫nel ngrok creado: {file_url}")

        # PASO 2: Analizar con OpenAI Vision
        logger.info(f"\nü§ñ Analizando imagen con OpenAI Vision...")
        logger.info(f"   - Prompt: {prompt[:100]}...")

        try:
            from openai import OpenAI

            # Obtener API key (buscar ambos nombres para compatibilidad)
            api_key = os.getenv('OPENAI_API_KEY') or os.getenv('OPEN_API_KEY')
            if not api_key:
                raise ValueError(
                    "OPENAI_API_KEY no configurada en .env"
                )

            openai_client = OpenAI(api_key=api_key)
            logger.info("   ‚úÖ Cliente OpenAI inicializado")

            # Llamar a OpenAI Vision con la URL p√∫blica
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": file_url}
                            }
                        ]
                    }
                ],
                max_tokens=2000
            )

            analysis_result = response.choices[0].message.content
            tokens_used = response.usage.total_tokens

            logger.info(f"   ‚úÖ An√°lisis completado")
            logger.info(f"   - Tokens usados: {tokens_used}")
            logger.info(f"   - Longitud: {len(analysis_result)} chars")

            return {
                "status": "success",
                "filename": filename,
                "public_url": file_url,
                "tunnel_url": public_url,
                "analysis": analysis_result,
                "prompt_used": prompt,
                "model": "gpt-4o-mini",
                "tokens_used": tokens_used,
                "local_path": str(file_path),
                "file_size_bytes": file_size,
                "file_extension": file_path.suffix,
                "timestamp": datetime.now().isoformat(),
                "note": "An√°lisis completado. URL p√∫blica activa."
            }

        except Exception as openai_error:
            logger.error(f"   ‚ùå Error OpenAI: {str(openai_error)}")
            return {
                "status": "partial_success",
                "filename": filename,
                "public_url": file_url,
                "tunnel_url": public_url,
                "error": f"An√°lisis fall√≥: {str(openai_error)}",
                "file_size_bytes": file_size,
                "note": "URL creada pero an√°lisis fall√≥"
            }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"   ‚ùå Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error: {str(e)}"
        )


@router.post(
    '/analyze-local-only',
    tags=["ai-analysis"],
    summary="An√°lisis con OpenAI desde run_id local (Base64 optimizado, sin GCS)"
)
async def analyze_local_only(
    run_id: str,
    top_n: int = 10
) -> Dict[str, Any]:
    """
    An√°lisis completo con OpenAI usando Base64.

    FLUJO SIMPLE:
    1. Seleccionar anuncios del CSV
    2. Descargar im√°genes ‚Üí Base64
    3. Extraer frames de videos ‚Üí Base64
    4. Enviar TODO a OpenAI

    Args:
        run_id: ID del run con datos locales
        top_n: N√∫mero de anuncios a analizar (default: 10)
    """
    logger.info("="*80)
    logger.info("üöÄ AN√ÅLISIS CON BASE64")
    logger.info("="*80)
    logger.info(f"üìã RUN_ID: {run_id}")
    logger.info(f"üìä Top N: {top_n}")

    try:
        # PASO 1: OpenAI
        logger.info("\nüì° PASO 1: Configurando OpenAI...")
        from openai import AsyncOpenAI
        import base64
        from io import BytesIO
        from PIL import Image

        api_key = os.getenv("OPENAI_API_KEY") or os.getenv("OPEN_API_KEY")
        if not api_key:
            raise HTTPException(503, "OPENAI_API_KEY no configurada")

        openai_client = AsyncOpenAI(api_key=api_key)
        logger.info("   ‚úÖ OpenAI configurado")

        # PASO 2: Localizar CSV
        logger.info("\nüìä PASO 2: Localizando dataset...")
        base_dir = get_facebook_saved_base()
        run_dir = base_dir / run_id
        csv_path = run_dir / f"{run_id}.csv"

        if not csv_path.exists():
            raise HTTPException(404, f"CSV no encontrado en {csv_path}")

        logger.info(f"   ‚úÖ CSV: {csv_path}")

        # PASO 3: Seleccionar anuncios
        logger.info("\nüìä PASO 3: Seleccionando anuncios...")
        import pandas as pd
        df = pd.read_csv(csv_path)

        logger.info(f"   üìÑ CSV cargado: {len(df)} filas")

        media_data = {}
        count = 0
        errores = 0

        for idx, row in df.iterrows():
            if count >= top_n:
                break

            ad_id = str(row.get('ad_archive_id')
                        or row.get('ad_id') or f'ad_{idx}')
            snapshot_str = row.get('snapshot', '{}')

            # NUEVA L√ìGICA: Extraer URLs directamente del snapshot sin parsear JSON
            img_urls = []
            vid_urls = []

            # Convertir snapshot a string y buscar URLs con regex
            if pd.notna(snapshot_str):
                snapshot_text = str(snapshot_str)

                # Buscar URLs de im√°genes (cualquier URL que parezca imagen)
                img_patterns = [
                    r'https?://[^\s"\',}]+\.(?:jpg|jpeg|png|webp)',
                    r'"(?:original_image_url|resized_image_url)":\s*"([^"]+)"',
                    r"'(?:original_image_url|resized_image_url)':\s*'([^']+)'"
                ]

                for pattern in img_patterns:
                    matches = re.findall(pattern, snapshot_text, re.IGNORECASE)
                    for match in matches[:3]:  # M√°ximo 3 im√°genes
                        url = match if isinstance(match, str) else match[0]
                        if url and url.startswith('http'):
                            img_urls.append(url)
                            if len(img_urls) >= 3:
                                break
                    if img_urls:
                        break

                # Buscar URLs de videos
                vid_patterns = [
                    r'https?://[^\s"\',}]+\.(?:mp4|mov|avi)',
                    r'"(?:video_hd_url|video_sd_url)":\s*"([^"]+)"',
                    r"'(?:video_hd_url|video_sd_url)':\s*'([^']+)'"
                ]

                for pattern in vid_patterns:
                    matches = re.findall(pattern, snapshot_text, re.IGNORECASE)
                    if matches:
                        url = matches[0] if isinstance(
                            matches[0], str) else matches[0][0]
                        if url and url.startswith('http'):
                            vid_urls.append(url)
                            break

            # Si encontramos multimedia, agregar
            if img_urls or vid_urls:
                media_data[ad_id] = {
                    'images': img_urls,
                    'videos': vid_urls
                }
                count += 1
                logger.info(
                    f"   ‚úì {ad_id}: {len(img_urls)} imgs, {len(vid_urls)} vids")
            else:
                errores += 1

        logger.info(f"   ‚úÖ {len(media_data)} anuncios con multimedia")
        logger.info(f"   ‚ö†Ô∏è  {errores} anuncios sin multimedia")

        if len(media_data) == 0:
            raise HTTPException(404, "No se encontr√≥ multimedia en el dataset")

        # PASO 4: Buscar archivos YA DESCARGADOS en storage
        logger.info("\nüì¶ PASO 4: Buscando archivos descargados...")

        # Directorio correcto: run_dir contiene media/ y opcionalmente video_frames/
        media_dir = run_dir / "media"
        video_frames_dir = run_dir / "video_frames"

        logger.info(f"   üìÅ Media: {media_dir}")

        if not media_dir.exists():
            raise HTTPException(
                404, f"Directorio media no existe: {media_dir}")

        # PASO 4.1: Extraer frames de videos encontrados en media/
        logger.info("\nüé¨ Extrayendo frames de videos...")
        
        # Buscar archivos de video en media/
        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.webm']
        video_files = [
            f for f in media_dir.iterdir()
            if f.is_file() and f.suffix.lower() in video_extensions
        ]
        
        if video_files:
            logger.info(f"   üìπ {len(video_files)} videos encontrados")
            
            # Crear directorio de frames si no existe
            video_frames_dir.mkdir(exist_ok=True)
            
            try:
                import cv2
                
                for video_path in video_files:
                    try:
                        logger.info(f"   üîÑ Procesando: {video_path.name}")
                        
                        cap = cv2.VideoCapture(str(video_path))
                        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                        
                        # Extraer 3 frames: inicio, medio, fin
                        frames_to_extract = [
                            0,  # Primer frame
                            frame_count // 2,  # Frame del medio
                            frame_count - 1  # √öltimo frame
                        ]
                        
                        # Nombre base del video (sin extensi√≥n)
                        base_name = video_path.stem
                        
                        extracted = 0
                        for i, frame_num in enumerate(frames_to_extract):
                            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_num)
                            ret, frame = cap.read()
                            
                            if ret:
                                # Guardar frame
                                frame_filename = (
                                    f"{base_name}_frame{i}.jpg"
                                )
                                frame_path = video_frames_dir / frame_filename
                                cv2.imwrite(str(frame_path), frame)
                                extracted += 1
                        
                        cap.release()
                        logger.info(
                            f"      ‚úÖ {extracted} frames extra√≠dos"
                        )
                        
                    except Exception as e:
                        logger.error(
                            f"      ‚ùå Error con {video_path.name}: {e}"
                        )
                        continue
                
            except ImportError:
                logger.warning(
                    "   ‚ö†Ô∏è  OpenCV no disponible, "
                    "no se pueden extraer frames"
                )
        else:
            logger.info("   ‚ÑπÔ∏è  No hay videos para procesar")

        # video_frames es opcional
        has_video_frames = video_frames_dir.exists()
        if has_video_frames:
            logger.info(f"   üìÅ Frames: {video_frames_dir}")
        else:
            logger.info(
                "   ‚ÑπÔ∏è  No hay directorio video_frames (anuncios sin videos)")

        # Cargar prompt desde archivo configurado
        prompt_file = os.getenv("PROMPT_FILE", "prompt_simple.txt")
        prompt_path = Path("prompts") / prompt_file

        if prompt_path.exists():
            with open(prompt_path, "r", encoding="utf-8") as f:
                prompt_template = f.read().strip()
            logger.info(f"   üìÑ Prompt cargado desde: {prompt_file}")
        else:
            # Fallback si no existe el archivo
            prompt_template = "Analiza estos anuncios de Facebook. Describe estrategia visual, colores, mensajes y elementos clave."
            logger.warning(
                f"   ‚ö†Ô∏è  Archivo {prompt_file} no encontrado, usando prompt por defecto")

        # Preparar informaci√≥n del dataset para el prompt
        dataset_info = f"""
INFORMACI√ìN DEL DATASET:
- Run ID: {run_id}
- Total de anuncios en CSV: {len(df)}
- Anuncios con multimedia: {len(media_data)}
- IDs de anuncios seleccionados: {', '.join(media_data.keys())}

INSTRUCCI√ìN CR√çTICA: Debes retornar √öNICAMENTE un objeto JSON v√°lido y completo.
No agregues texto adicional antes o despu√©s del JSON.
El JSON debe seguir EXACTAMENTE la estructura solicitada en el prompt.
Genera un an√°lisis COMPLETO Y DETALLADO para cada anuncio.
"""

        content_blocks = []
        content_blocks.append({
            "type": "text",
            "text": dataset_info + "\n\n" + prompt_template
        })

        total_imgs = 0
        total_video_frames = 0

        for ad_id in media_data.keys():
            logger.info(f"   üìã Anuncio {ad_id}")
            
            # Separar im√°genes est√°ticas de frames de video
            static_images = []
            video_frames = []

            # Buscar en media/ - im√°genes est√°ticas
            for img_file in media_dir.iterdir():
                if img_file.is_file() and img_file.name.startswith(str(ad_id)):
                    if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.webp']:
                        static_images.append(img_file)

            # Buscar frames de video solo si existe el directorio
            if has_video_frames:
                for frame_file in video_frames_dir.iterdir():
                    if frame_file.is_file() and frame_file.name.startswith(str(ad_id)):
                        if frame_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                            video_frames.append(frame_file)

            logger.info(
                f"      üìÅ {len(static_images)} imgs est√°ticas, "
                f"{len(video_frames)} frames de video"
            )

            # Agregar header del anuncio con tipo de contenido
            ad_header = f"\n{'='*60}\nANUNCIO ID: {ad_id}\n"
            if static_images:
                ad_header += f"- IM√ÅGENES EST√ÅTICAS: {len(static_images)}\n"
            if video_frames:
                ad_header += f"- VIDEO (frames extra√≠dos): {len(video_frames)}\n"
            ad_header += f"{'='*60}\n"
            
            content_blocks.append({
                "type": "text",
                "text": ad_header
            })

            # Procesar TODAS las im√°genes est√°ticas
            if static_images:
                content_blocks.append({
                    "type": "text",
                    "text": "\nüì∑ IM√ÅGENES EST√ÅTICAS:\n"
                })
                
                for img_path in static_images:
                    try:
                        with Image.open(img_path) as img:
                            if img.mode in ('RGBA', 'P'):
                                img = img.convert('RGB')
                            if max(img.size) > 800:
                                img.thumbnail(
                                    (800, 800),
                                    Image.Resampling.LANCZOS
                                )
                            buffered = BytesIO()
                            img.save(
                                buffered,
                                format="JPEG",
                                quality=85,
                                optimize=True
                            )
                            b64 = base64.b64encode(
                                buffered.getvalue()
                            ).decode('utf-8')

                            content_blocks.append({
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{b64}",
                                    "detail": "low"
                                }
                            })
                            total_imgs += 1
                            logger.info(f"      ‚úì IMG: {img_path.name}")
                    except Exception as e:
                        logger.error(
                            f"      ‚úó Error en {img_path.name}: {e}"
                        )

            # Procesar TODOS los frames de video
            if video_frames:
                content_blocks.append({
                    "type": "text",
                    "text": (
                        "\nüé• FRAMES DE VIDEO "
                        "(extra√≠dos del anuncio en video):\n"
                    )
                })
                
                for frame_path in video_frames:
                    try:
                        with Image.open(frame_path) as img:
                            if img.mode in ('RGBA', 'P'):
                                img = img.convert('RGB')
                            if max(img.size) > 800:
                                img.thumbnail(
                                    (800, 800),
                                    Image.Resampling.LANCZOS
                                )
                            buffered = BytesIO()
                            img.save(
                                buffered,
                                format="JPEG",
                                quality=85,
                                optimize=True
                            )
                            b64 = base64.b64encode(
                                buffered.getvalue()
                            ).decode('utf-8')

                            content_blocks.append({
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{b64}",
                                    "detail": "low"
                                }
                            })
                            total_video_frames += 1
                            logger.info(
                                f"      ‚úì VIDEO FRAME: {frame_path.name}"
                            )
                    except Exception as e:
                        logger.error(
                            f"      ‚úó Error en {frame_path.name}: {e}"
                        )

        # PASO 5: Enviar a OpenAI
        logger.info("\nüöÄ PASO 5: Enviando a OpenAI...")
        logger.info(
            f"   üìä Total: {total_imgs} im√°genes est√°ticas + "
            f"{total_video_frames} frames de video"
        )

        total_assets = total_imgs + total_video_frames
        if total_assets == 0:
            raise HTTPException(400, "No se proces√≥ ninguna multimedia")

        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": content_blocks}],
            max_tokens=16000  # Permitir respuestas mucho m√°s largas
        )

        analysis = response.choices[0].message.content
        tokens_used = response.usage.total_tokens

        logger.info(f"   ‚úÖ Completado - {tokens_used} tokens")

        return {
            "status": "success",
            "run_id": run_id,
            "analyzed_ads": len(media_data),
            "total_images_processed": total_imgs,
            "total_video_frames_processed": total_video_frames,
            "total_assets": total_assets,
            "analysis": analysis,
            "tokens_used": tokens_used,
            "timestamp": datetime.now().isoformat()
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"   ‚ùå Error: {str(e)}")
        raise HTTPException(500, f"Error: {str(e)}")


@router.post(
    '/analyze-and-generate-pdf',
    tags=["ai-analysis"],
    summary="An√°lisis con OpenAI + Generaci√≥n de PDF (Base64 optimizado)"
)
async def analyze_and_generate_pdf(
    run_id: str,
    top_n: int = 10
) -> Dict[str, Any]:
    """
    An√°lisis completo con OpenAI + generaci√≥n de PDF profesional.

    Flujo:
    1. Analiza anuncios con OpenAI (igual que analyze-local-only)
    2. Parsea el JSON de respuesta
    3. Genera un PDF profesional con ReportLab
    4. Retorna la ruta del PDF generado

    Args:
        run_id: ID del run con datos locales
        top_n: N√∫mero de anuncios a analizar (default: 10)
    """
    logger.info("="*80)
    logger.info("üöÄ AN√ÅLISIS + GENERACI√ìN PDF")
    logger.info("="*80)

    try:
        # Ejecutar an√°lisis (reutilizar l√≥gica de analyze-local-only)
        analysis_result = await analyze_local_only(run_id, top_n)

        # Verificar que el an√°lisis fue exitoso
        if analysis_result.get("status") != "success":
            raise HTTPException(500, "An√°lisis fall√≥")

        analysis_text = analysis_result.get("analysis", "")

        logger.info("\nüìÑ Procesando respuesta de OpenAI...")

        # Buscar JSON en la respuesta (puede estar envuelto en markdown)
        # Primero intentar extraer de bloques ```json
        json_pattern = r'```json\s*(\{.*?\})\s*```'
        json_match = re.search(json_pattern, analysis_text, re.DOTALL)

        if not json_match:
            # Si no hay bloque markdown, buscar JSON directamente
            json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)

        if json_match:
            if json_match.lastindex:  # Tiene grupo de captura
                json_str = json_match.group(1)
            else:
                json_str = json_match.group(0)

            # Intentar parsear JSON con json-repair
            try:
                # Primero intento con json est√°ndar
                analysis_data = json.loads(json_str)
                logger.info("   ‚úÖ JSON parseado correctamente")
            except json.JSONDecodeError as e:
                logger.warning(
                    f"   ‚ö†Ô∏è  JSON malformado (pos {e.pos}), "
                    f"reparando con json-repair..."
                )

                try:
                    # Usar json-repair para reparar JSON autom√°ticamente
                    # repair_json retorna directamente el objeto Python
                    repaired_data = repair_json(json_str)

                    # Asegurar que sea un diccionario
                    if isinstance(repaired_data, dict):
                        analysis_data = repaired_data
                        logger.info("   ‚úÖ JSON reparado exitosamente")
                    else:
                        raise ValueError(
                            f"repair_json retorn√≥ {type(repaired_data)}"
                        )
                except Exception as repair_error:
                    logger.error(
                        f"   ‚ùå json-repair fall√≥: {str(repair_error)}"
                    )
                    # Crear estructura b√°sica
                    overview_text = (
                        analysis_text[:300]
                        if len(analysis_text) > 300
                        else analysis_text
                    )
                    analysis_data = {
                        "metadata": {
                            "campaign_name": f"An√°lisis {run_id}",
                            "total_ads_analyzed": top_n
                        },
                        "executive_summary": {
                            "overview": overview_text,
                            "key_findings": (
                                "An√°lisis parcial - JSON incompleto"
                            ),
                            "strategic_implications": (
                                "Se recomienda reducir el n√∫mero de "
                                "anuncios o simplificar el prompt"
                            )
                        },
                        "assets_analysis": [],
                        "global_conclusions": {
                            "summary": (
                                "An√°lisis generado pero respuesta "
                                "incompleta de OpenAI"
                            )
                        }
                    }
        else:
            # Si no hay JSON, crear estructura b√°sica
            logger.warning(
                "   ‚ö†Ô∏è  No se encontr√≥ JSON, usando estructura b√°sica")
            overview_text = (
                analysis_text[:300]
                if len(analysis_text) > 300
                else analysis_text
            )
            analysis_data = {
                "metadata": {
                    "campaign_name": f"An√°lisis {run_id}",
                    "total_ads_analyzed": top_n
                },
                "executive_summary": {
                    "overview": overview_text,
                    "key_findings": (
                        "Respuesta de OpenAI sin formato JSON"
                    ),
                    "strategic_implications": "Texto plano convertido"
                },
                "assets_analysis": [],
                "global_conclusions": {
                    "summary": analysis_text
                }
            }

        # Guardar JSON y Markdown
        logger.info("\nüíæ Guardando resultados...")
        
        # Crear directorio de reportes si no existe
        reports_dir = get_facebook_saved_base() / "reports"
        reports_dir.mkdir(exist_ok=True)

        # Guardar JSON
        json_filename = f"{run_id}_analysis.json"
        json_path = reports_dir / json_filename
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"   ‚úÖ JSON guardado: {json_path}")

        # Guardar respuesta completa en Markdown estructurado
        md_filename = f"{run_id}_analysis.md"
        md_path = reports_dir / md_filename
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(f"# An√°lisis de Campa√±a: {run_id}\n\n")
            fecha = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            f.write(f"**Fecha:** {fecha}\n\n")
            ads = analysis_result.get('analyzed_ads', 0)
            f.write(f"**Anuncios analizados:** {ads}\n\n")
            imgs = analysis_result.get('total_images_processed', 0)
            frames = analysis_result.get('total_video_frames_processed', 0)
            f.write(f"**Im√°genes est√°ticas:** {imgs}\n\n")
            f.write(f"**Frames de video:** {frames}\n\n")
            tokens = analysis_result.get('tokens_used', 0)
            f.write(f"**Tokens usados:** {tokens}\n\n")
            f.write("---\n\n")
            
            # Extraer y destacar secci√≥n de comparaci√≥n si existe
            if isinstance(analysis_data, dict):
                comp = analysis_data.get('comparative_analysis')
                if comp:
                    f.write("## üèÜ AN√ÅLISIS COMPARATIVO\n\n")
                    
                    # Ganador
                    winner = comp.get('winner', {})
                    if winner:
                        f.write(f"### ü•á GANADOR: {winner.get('asset_id')}\n\n")
                        f.write(f"**Razones:** {winner.get('reasons')}\n\n")
                        strengths = winner.get('key_strengths', [])
                        if strengths:
                            f.write("**Fortalezas clave:**\n")
                            for s in strengths:
                                f.write(f"- {s}\n")
                            f.write("\n")
                    
                    # Runner up
                    runner = comp.get('runner_up', {})
                    if runner:
                        runner_id = runner.get('asset_id')
                        f.write(f"### ü•à SEGUNDO LUGAR: {runner_id}\n\n")
                        f.write(f"{runner.get('reasons')}\n\n")
                    
                    # Tabla de ranking
                    ranking = comp.get('ranking_table', [])
                    if ranking:
                        f.write("### üìä TABLA DE RANKING\n\n")
                        f.write(
                            "| Rank | Anuncio ID | Score | "
                            "Mejor Atributo |\n"
                        )
                        f.write(
                            "|------|------------|-------|"
                            "----------------|\n"
                        )
                        for r in ranking:
                            rank = r.get('rank', '')
                            aid = r.get('asset_id', '')
                            score = r.get('overall_score', '')
                            attr = r.get('best_attribute', '')
                            f.write(f"| {rank} | {aid} | {score} | {attr} |\n")
                        f.write("\n")
                    
                    f.write("---\n\n")
            
            f.write("## üìÑ Respuesta Completa de OpenAI\n\n")
            f.write(analysis_text)
        
        logger.info(f"   ‚úÖ Markdown guardado: {md_path}")

        return {
            "status": "success",
            "run_id": run_id,
            "analyzed_ads": analysis_result.get("analyzed_ads", 0),
            "total_images_processed": analysis_result.get(
                "total_images_processed", 0
            ),
            "tokens_used": analysis_result.get("tokens_used", 0),
            "json_path": str(json_path),
            "json_filename": json_filename,
            "markdown_path": str(md_path),
            "markdown_filename": md_filename,
            "analysis_data": analysis_data,
            "timestamp": datetime.now().isoformat()
        }

    except HTTPException:
        raise
    except json.JSONDecodeError as e:
        logger.error(f"   ‚ùå Error parseando JSON: {str(e)}")
        raise HTTPException(
            500, f"Error parseando respuesta de OpenAI: {str(e)}")
    except Exception as e:
        logger.error(f"   ‚ùå Error: {str(e)}")
        raise HTTPException(500, f"Error: {str(e)}")


@router.post(
    '/json-to-pdf',
    tags=["ai-analysis"],
    summary="Convierte JSON de an√°lisis a PDF usando OpenAI"
)
async def json_to_pdf(run_id: str) -> Dict[str, Any]:
    """
    Toma el JSON guardado de un an√°lisis previo y genera un PDF
    profesional usando OpenAI para formatear y estructurar el contenido.
    
    Args:
        run_id: ID del run con el JSON de an√°lisis guardado
    
    Returns:
        Informaci√≥n del PDF generado
    """
    logger.info("="*80)
    logger.info(f"üìÑ GENERANDO PDF DESDE JSON: {run_id}")
    logger.info("="*80)
    
    try:
        # Buscar archivo JSON
        reports_dir = get_facebook_saved_base() / "reports"
        json_path = reports_dir / f"{run_id}_analysis.json"
        
        if not json_path.exists():
            raise HTTPException(
                404,
                f"No se encontr√≥ an√°lisis para run_id: {run_id}"
            )
        
        # Cargar JSON
        logger.info(f"\nüìÇ Cargando JSON: {json_path}")
        with open(json_path, 'r', encoding='utf-8') as f:
            analysis_data = json.load(f)
        
        logger.info("   ‚úÖ JSON cargado correctamente")
        
        # Usar OpenAI para estructurar contenido en formato Markdown
        logger.info("\nü§ñ Usando OpenAI para formatear reporte...")
        
        from openai import AsyncOpenAI
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise HTTPException(500, "OPENAI_API_KEY no configurada")
        
        client = AsyncOpenAI(api_key=api_key)
        
        # Prompt para formatear el JSON en Markdown estructurado
        format_prompt = f"""
Eres un editor profesional de reportes de marketing.
Toma este an√°lisis en JSON y convi√©rtelo en un reporte Markdown
profesional, bien estructurado y f√°cil de leer.

Estructura requerida:
1. T√≠tulo principal y metadata
2. Resumen ejecutivo
3. An√°lisis por activo (cada imagen analizada)
4. An√°lisis cruzado
5. Conclusiones globales
6. Hoja de ruta estrat√©gica

Usa formato Markdown con:
- Headers apropiados (# ## ###)
- Listas y vi√±etas
- Tablas para scores
- √ânfasis en puntos clave (**bold**)
- Separadores visuales (---)

JSON:
{json.dumps(analysis_data, indent=2, ensure_ascii=False)}

Genera SOLO el Markdown, sin explicaciones adicionales.
"""

        response = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": format_prompt
                }
            ],
            max_tokens=8000,
            temperature=0.3
        )
        
        markdown_content = response.choices[0].message.content
        tokens_used = response.usage.total_tokens
        
        logger.info(f"   ‚úÖ Markdown generado ({tokens_used} tokens)")
        
        # Guardar Markdown formateado
        md_formatted_path = reports_dir / f"{run_id}_formatted.md"
        with open(md_formatted_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        logger.info(f"   ‚úÖ Markdown guardado: {md_formatted_path}")
        
        # Convertir Markdown a PDF usando markdown-pdf
        logger.info("\nüìÑ Convirtiendo a PDF...")
        
        try:
            from markdown_pdf import MarkdownPdf, Section
            
            pdf_filename = f"{run_id}_report.pdf"
            pdf_path = reports_dir / pdf_filename
            
            pdf = MarkdownPdf()
            pdf.add_section(Section(markdown_content))
            pdf.save(str(pdf_path))
            
            logger.info(f"   ‚úÖ PDF generado: {pdf_path}")
            
            return {
                "status": "success",
                "run_id": run_id,
                "json_path": str(json_path),
                "markdown_path": str(md_formatted_path),
                "pdf_path": str(pdf_path),
                "pdf_filename": pdf_filename,
                "tokens_used": tokens_used,
                "timestamp": datetime.now().isoformat()
            }
            
        except ImportError:
            # Si no est√° markdown-pdf, intentar con ReportLab directamente
            logger.warning(
                "   ‚ö†Ô∏è  markdown-pdf no disponible, "
                "usando conversi√≥n simplificada"
            )
            
            from reportlab.lib.pagesizes import letter
            from reportlab.lib.styles import getSampleStyleSheet
            from reportlab.platypus import (
                SimpleDocTemplate, Paragraph, Spacer
            )
            from reportlab.lib.units import inch
            
            pdf_filename = f"{run_id}_report.pdf"
            pdf_path = reports_dir / pdf_filename
            
            doc = SimpleDocTemplate(
                str(pdf_path),
                pagesize=letter,
                rightMargin=72,
                leftMargin=72,
                topMargin=72,
                bottomMargin=18
            )
            
            styles = getSampleStyleSheet()
            story = []
            
            # Convertir Markdown a p√°rrafos simples
            for line in markdown_content.split('\n'):
                if line.strip():
                    # Remover sintaxis markdown b√°sica
                    clean_line = line.replace('**', '').replace('##', '')
                    clean_line = clean_line.replace('#', '')
                    story.append(Paragraph(clean_line, styles['Normal']))
                    story.append(Spacer(1, 0.2*inch))
            
            doc.build(story)
            
            logger.info(f"   ‚úÖ PDF simple generado: {pdf_path}")
            
            return {
                "status": "success",
                "run_id": run_id,
                "json_path": str(json_path),
                "markdown_path": str(md_formatted_path),
                "pdf_path": str(pdf_path),
                "pdf_filename": pdf_filename,
                "tokens_used": tokens_used,
                "method": "simplified",
                "timestamp": datetime.now().isoformat()
            }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"   ‚ùå Error: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(500, f"Error generando PDF: {str(e)}")


@router.post(
    '/upload-local-files',
    tags=["ai-analysis"],
    summary="Sube m√∫ltiples archivos y expone con ngrok"
)
async def upload_local_files(files: list[UploadFile] = File(...)):
    """
    Sube archivos locales y los expone con URLs p√∫blicas via ngrok.
    """
    temp_dir = "temp_uploaded_files"
    os.makedirs(temp_dir, exist_ok=True)
    saved_files = []

    for file in files:
        file_path = os.path.join(temp_dir, file.filename)
        with open(file_path, "wb") as f:
            f.write(await file.read())
        saved_files.append(file_path)

    # Iniciar servidor local
    port = 8000
    httpd, thread = start_local_file_server(temp_dir, port)

    # Exponer con ngrok
    public_url = ngrok.connect(port)

    # Construir URLs p√∫blicas
    public_file_urls = [
        f"{public_url}/{os.path.basename(f)}" for f in saved_files
    ]

    return {
        "status": "success",
        "public_url": public_url,
        "file_urls": public_file_urls,
        "files_count": len(saved_files),
        "note": "URLs activas mientras el servidor est√© corriendo"
    }
