"""
AI Analysis Routes
Endpoints para an√°lisis completo con IA (Gemini)
"""
import threading
import socketserver
import http.server
from fastapi import APIRouter, HTTPException, UploadFile, File
from typing import Dict, Any
import os
from pyngrok import ngrok
from datetime import datetime
import logging
from pathlib import Path

from ..models.schemas import AnalyzeCampaignRequest
from ..utils.config import get_facebook_saved_base, get_gcs_service
from ..analysis import (
    scrape_and_prepare_run,
    analyze_campaign_with_gemini,
    compile_latex_to_pdf,
    build_manifest_from_gcs
)
from ..analysis.pdf_generator import create_pdf_from_analysis
from ..analysis.prompts import DEFAULT_PROMPT

# Configurar logger
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


# Servidor HTTP para archivos locales


def start_local_file_server(directory, port=8000):
    handler = http.server.SimpleHTTPRequestHandler
    os.chdir(directory)
    httpd = socketserver.TCPServer(("", port), handler)
    thread = threading.Thread(target=httpd.serve_forever, daemon=True)
    thread.start()
    return httpd, thread


@router.post('/upload-local-files', tags=["ai-analysis"], summary="Sube archivos locales y expone URLs p√∫blicas para AI")
async def upload_local_files(files: list[UploadFile] = File(...)):
    """
    Sube archivos locales, los expone por HTTP y retorna URLs p√∫blicas para an√°lisis con OpenAI Vision.
    """
    # Carpeta temporal para guardar archivos
    temp_dir = "temp_uploaded_files"
    os.makedirs(temp_dir, exist_ok=True)
    saved_files = []
    for file in files:
        file_path = os.path.join(temp_dir, file.filename)
        with open(file_path, "wb") as f:
            f.write(await file.read())
        saved_files.append(file_path)

    # Iniciar servidor local en el directorio de archivos
    port = 8000
    httpd, thread = start_local_file_server(temp_dir, port)

    # Exponer el puerto con pyngrok
    public_url = ngrok.connect(port)

    # Construir URLs p√∫blicas para cada archivo
    public_file_urls = [
        f"{public_url}/{os.path.basename(f)}" for f in saved_files]

    return {
        "status": "success",
        "public_url": public_url,
        "file_urls": public_file_urls,
        "note": "Las URLs estar√°n activas mientras el t√∫nel y el servidor local est√©n corriendo."
    }


@router.post(
    '/analyze-campaign-from-bucket',
    tags=["ai-analysis"],
    summary="An√°lisis con IA desde Bucket GCS"
)
async def analyze_campaign_from_bucket(run_id: str) -> Dict[str, Any]:
    """
    An√°lisis completo con IA desde archivos ya subidos en GCS

    **Uso simple**: Solo proporciona el run_id de archivos ya en el bucket

    Este endpoint:
    1. Verifica que el run_id exista en Google Cloud Storage
    2. Obtiene las URLs p√∫blicas de los archivos del manifest
    3. Analiza con Gemini AI usando el prompt configurado
    4. Genera reporte JSON + c√≥digo LaTeX
    5. Compila LaTeX a PDF autom√°ticamente
    6. Guarda resultados localmente

    Args:
        run_id: ID del run cuyos archivos est√°n en GCS

    Returns:
        JSON con an√°lisis completo, scores, recomendaciones y c√≥digo LaTeX

    Tiempo estimado: 30-90 segundos (solo an√°lisis con IA)
    """
    logger.info("="*80)
    logger.info("üöÄ INICIANDO AN√ÅLISIS DESDE BUCKET GCS")
    logger.info("="*80)
    logger.info(f"üìã Run ID: {run_id}")

    try:
        # PASO 1: Verificar Gemini
        logger.info("\nüì° PASO 1/6: Verificando servicio Gemini AI...")
        try:
            from app.services.gemini_service import GeminiService
            gemini_service = GeminiService()
            logger.info("   ‚úÖ Gemini AI disponible")
        except Exception as e:
            logger.error(f"   ‚ùå Error verificando Gemini: {str(e)}")
            raise HTTPException(
                status_code=503,
                detail=f"Gemini AI not available: {str(e)}"
            )

        # PASO 2: Verificar GCS
        logger.info("\n‚òÅÔ∏è  PASO 2/6: Verificando Google Cloud Storage...")
        gcs_service = get_gcs_service()
        if gcs_service is None:
            logger.error("   ‚ùå GCS service no configurado")
            raise HTTPException(
                status_code=503,
                detail="GCS service not configured"
            )
        logger.info("   ‚úÖ GCS service disponible")

        # PASO 3: Construir manifest desde GCS
        logger.info("\nüì¶ PASO 3/6: Construyendo manifest desde bucket...")
        logger.info(f"   - Buscando archivos en bucket para run: {run_id}")
        try:
            manifest_data = build_manifest_from_gcs(run_id, gcs_service)
            logger.info(f"   ‚úÖ Manifest construido")
            logger.info(
                f"   ‚úÖ Anuncios encontrados: {len(manifest_data.get('ads', []))}")
        except Exception as e:
            logger.error(f"   ‚ùå Error construyendo manifest: {str(e)}")
            raise HTTPException(
                status_code=404,
                detail=f"Error obteniendo datos del bucket: {str(e)}"
            )

        # PASO 4: Analizar con Gemini
        logger.info("\nü§ñ PASO 4/6: Analizando con Gemini AI...")
        try:
            analysis_prompt = os.getenv('PROMPT', DEFAULT_PROMPT)
            logger.info(
                f"   - Usando prompt por defecto ({len(analysis_prompt)} chars)")

            base = get_facebook_saved_base()
            reports_dir = base / 'reports_json'

            analysis_json = analyze_campaign_with_gemini(
                run_id=run_id,
                manifest_data=manifest_data,
                analysis_prompt=analysis_prompt,
                gemini_service=gemini_service,
                reports_dir=reports_dir,
                source="gcs_bucket"
            )
            logger.info("   ‚úÖ An√°lisis completado")
        except Exception as e:
            logger.error(f"   ‚ùå Error en an√°lisis: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Error en an√°lisis IA: {str(e)}"
            )

        # PASO 5: Guardar resultados
        logger.info("\nüíæ PASO 5/6: Guardando resultados...")
        try:
            from ..analysis.gemini_analyzer import save_analysis_results

            saved_files = save_analysis_results(
                run_id=run_id,
                analysis_json=analysis_json,
                reports_dir=reports_dir,
                source="gcs_bucket"
            )
            logger.info(f"   ‚úÖ JSON: {saved_files['report_filename']}")
            logger.info(f"   ‚úÖ LaTeX: {saved_files['latex_filename']}")
        except Exception as e:
            logger.error(f"   ‚ùå Error guardando: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Error guardando archivos: {str(e)}"
            )

        # PASO 6: Compilar PDF
        logger.info("\nüìÑ PASO 6/6: Compilando PDF...")
        pdf_filename = None
        pdf_generated = False
        pdf_error = None

        if saved_files['latex_path']:
            try:
                pdf_result = compile_latex_to_pdf(
                    saved_files['latex_path'],
                    reports_dir
                )
                pdf_filename = pdf_result['pdf_filename']
                pdf_generated = pdf_result['success']
                pdf_error = pdf_result['error']

                if pdf_generated:
                    logger.info(f"   ‚úÖ PDF: {pdf_filename}")
                else:
                    logger.warning(f"   ‚ö†Ô∏è  PDF no generado: {pdf_error}")
            except Exception as e:
                logger.error(f"   ‚ùå Error PDF: {str(e)}")
                pdf_error = str(e)

        logger.info("\n" + "="*80)
        logger.info("üéâ AN√ÅLISIS DESDE BUCKET COMPLETADO")
        logger.info("="*80)

        return {
            'status': 'success',
            'run_id': run_id,
            'total_ads_analyzed': len(manifest_data.get('ads', [])),
            'report_path': str(saved_files['report_path']),
            'report_filename': saved_files['report_filename'],
            'latex_file': saved_files['latex_filename'],
            'pdf_file': pdf_filename,
            'pdf_generated': pdf_generated,
            'pdf_error': pdf_error,
            'analysis_summary': {
                'best_performer': analysis_json.get(
                    'campaign_summary', {}
                ).get('best_performer'),
                'generated_at': datetime.now().isoformat()
            },
            'full_analysis': analysis_json
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"\n‚ùå ERROR INESPERADO: {str(e)}")
        logger.exception("Traceback:")
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing from bucket: {str(e)}"
        )


@router.post(
    '/analyze-campaign-with-ai',
    tags=["ai-analysis"],
    summary="An√°lisis Completo con IA y archivos locales"
)
async def analyze_campaign_with_ai(
    request: AnalyzeCampaignRequest,
    files: list[UploadFile] = File(None)
) -> Dict[str, Any]:
    """
    An√°lisis completo end-to-end de campa√±a con IA

    **Uso simple**: Solo pega la URL de Facebook Ads Library y listo!

    Flujo autom√°tico:
    1. Scrape anuncios desde la URL
    2. Identifica los top N mejores anuncios (por score heur√≠stico)
    3. Descarga archivos multimedia
    4. Sube todo a Google Cloud Storage
    5. Analiza con Gemini AI (prompt personalizable)
    6. Genera reporte JSON + c√≥digo LaTeX para PDF
    7. Compila LaTeX a PDF autom√°ticamente
    8. Guarda resultados localmente

    Args:
        request: {
            "url": "https://facebook.com/ads/library/?id=...",
            "custom_prompt": "Analiza como experto...",  # Opcional
            "count": 100,  # Opcional (default: 100)
            "top": 10  # Opcional (default: 10)
        }

    Returns:
        JSON con an√°lisis completo, scores, recomendaciones y c√≥digo LaTeX

    Tiempo estimado: 2-5 minutos
    """
    logger.info("="*80)
    logger.info("üöÄ INICIANDO AN√ÅLISIS COMPLETO DE CAMPA√ëA CON IA")
    logger.info("="*80)
    logger.info(f"üìã Par√°metros recibidos:")
    logger.info(f"   - URL: {request.url}")

    logger.info(f"   - Count: {request.count or 100}")
    logger.info(f"   - Top: {request.top or 10}")
    logger.info(
        f"   - Custom prompt: {'S√≠' if request.custom_prompt else 'No'}")
    if files:
        logger.info(f"   - Archivos locales recibidos: {len(files)}")

    try:
        # PASO 1: Verificar OpenAI
        logger.info("\nüì° PASO 1/7: Verificando servicio OpenAI...")
        try:
            from openai import OpenAI

            api_key = os.getenv("OPEN_API_KEY")
            if not api_key:
                raise HTTPException(
                    status_code=503,
                    detail="OPEN_API_KEY no configurada en .env"
                )

            openai_client = OpenAI(api_key=api_key)
            logger.info("   ‚úÖ OpenAI disponible y configurado")
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"   ‚ùå Error verificando OpenAI: {str(e)}")
            raise HTTPException(
                status_code=503,
                detail=f"OpenAI not available: {str(e)}"
            )

        # PASO EXTRA: Si hay archivos locales, exponerlos y obtener URLs p√∫blicas
        public_file_urls = []
        if files:
            temp_dir = "temp_uploaded_files"
            os.makedirs(temp_dir, exist_ok=True)
            saved_files = []
            for file in files:
                file_path = os.path.join(temp_dir, file.filename)
                with open(file_path, "wb") as f:
                    f.write(await file.read())
                saved_files.append(file_path)

            port = 8000
            httpd, thread = start_local_file_server(temp_dir, port)
            public_url = ngrok.connect(port)
            public_file_urls = [
                f"{public_url}/{os.path.basename(f)}" for f in saved_files]
            logger.info(f"   ‚úÖ Archivos locales expuestos en: {public_url}")

        # PASO 2: Ejecutar workflow completo (scrape + prepare)
        logger.info("\nüîç PASO 2/7: Ejecutando scrape y preparaci√≥n...")
        logger.info("   - Extrayendo anuncios de Facebook")
        logger.info("   - Seleccionando top mejores anuncios")
        logger.info("   - Descargando multimedia")
        try:
            workflow_result = await scrape_and_prepare_run(
                url=request.url,
                count=request.count or 100,
                top=request.top or 10
            )
            run_id = workflow_result['run_id']
            manifest_data = workflow_result['manifest']
            logger.info(f"   ‚úÖ Scrape completado - Run ID: {run_id}")
            logger.info(
                f"   ‚úÖ Anuncios procesados: {len(manifest_data.get('ads', []))}")
        except Exception as e:
            logger.error(f"   ‚ùå Error en scrape/preparaci√≥n: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Error en scrape: {str(e)}"
            )

        # PASO 3: Preparar prompt
        logger.info("\nüìù PASO 3/7: Preparando prompt de an√°lisis...")
        custom_prompt = request.custom_prompt
        env_prompt = os.getenv('PROMPT', DEFAULT_PROMPT)
        analysis_prompt = custom_prompt or env_prompt
        prompt_source = "personalizado" if custom_prompt else "por defecto"
        logger.info(f"   ‚úÖ Usando prompt {prompt_source}")
        logger.info(
            f"   ‚úÖ Longitud del prompt: {len(analysis_prompt)} caracteres")

        # PASO 4: Analizar con OpenAI
        logger.info("\nü§ñ PASO 4/7: Analizando campa√±a con OpenAI Vision...")
        logger.info("   - Enviando datos a OpenAI (GPT-4 Vision)")
        logger.info("   - Esperando an√°lisis detallado...")
        try:
            base = get_facebook_saved_base()
            reports_dir = base / 'reports_json'

            # Construir mensaje √∫nico con todas las im√°genes
            content = [
                {
                    "type": "text",
                    "text": f"{analysis_prompt}\n\nAnaliza los siguientes anuncios:"
                }
            ]

            # Agregar cada anuncio con sus im√°genes
            for ad in manifest_data.get('ads', []):
                ad_id = ad.get('ad_id', 'N/A')
                files_ad = ad.get('files', [])

                content.append({
                    "type": "text",
                    "text": f"\n--- Anuncio ID: {ad_id} ---"
                })

                # Agregar im√°genes del anuncio
                image_count = 0
                for file_info in files_ad:
                    if file_info.get('type') == 'image':
                        content.append({
                            "type": "image_url",
                            "image_url": {
                                "url": file_info['url']
                            }
                        })
                        image_count += 1

                logger.info(f"   - Anuncio {ad_id}: {image_count} im√°genes")

            # Si hay archivos locales, agregarlos tambi√©n
            if public_file_urls:
                for url in public_file_urls:
                    content.append({
                        "type": "image_url",
                        "image_url": {"url": url}
                    })
                logger.info(
                    f"   - Im√°genes locales agregadas al an√°lisis: {len(public_file_urls)}")

            messages = [
                {
                    "role": "user",
                    "content": content
                }
            ]

            # Llamar a OpenAI GPT-4 Vision (modelo actualizado)
            ads_count = len(manifest_data.get('ads', []))
            logger.info(f"   - Total anuncios: {ads_count}")
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=4096,
                temperature=0.5  # Balance precisi√≥n-creatividad para an√°lisis
            )

            analysis_text = response.choices[0].message.content
            analysis_json = {
                "run_id": run_id,
                "analysis": analysis_text,
                "model": "gpt-4o",
                "ads_count": len(manifest_data.get('ads', [])),
                "manifest": manifest_data,
                "local_files": public_file_urls
            }

            logger.info("   ‚úÖ An√°lisis con IA completado")
            logger.info("   ‚úÖ Respuesta recibida de OpenAI")
        except Exception as e:
            logger.error(f"   ‚ùå Error en an√°lisis con OpenAI: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Error en an√°lisis IA: {str(e)}"
            )

        # PASO 5: Guardar resultados
        logger.info("\nüíæ PASO 5/7: Guardando resultados...")
        try:
            import json
            reports_dir.mkdir(parents=True, exist_ok=True)

            # Guardar JSON
            report_filename = f"{run_id}_analysis.json"
            report_path = reports_dir / report_filename
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_json, f, ensure_ascii=False, indent=2)

            saved_files = {
                'report_filename': report_filename,
                'report_path': str(report_path),
                'latex_filename': None
            }

            logger.info(f"   ‚úÖ JSON guardado: {report_filename}")
            logger.info(f"   ‚úÖ Ruta: {report_path}")
        except Exception as e:
            logger.error(f"   ‚ùå Error guardando resultados: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Error guardando archivos: {str(e)}"
            )

        # PASO 6: (Saltado - no se genera LaTeX/PDF con OpenAI)
        logger.info("\nüìÑ PASO 6/7: Generaci√≥n de PDF...")
        logger.info("   ‚ö†Ô∏è  Paso omitido (solo se genera JSON con OpenAI)")

        # PASO 7: Resultado final
        logger.info("\n‚ú® PASO 7/7: Preparando respuesta final...")
        logger.info("="*80)
        logger.info("üéâ AN√ÅLISIS COMPLETADO EXITOSAMENTE")
        logger.info("="*80)
        logger.info("üìä Resumen:")
        ads_count = len(manifest_data.get('ads', []))
        logger.info(f"   - Run ID: {run_id}")
        logger.info(f"   - Anuncios analizados: {ads_count}")
        logger.info("   - Reporte JSON: ‚úÖ")
        logger.info("   - Proveedor: OpenAI (gpt-3.5-turbo)")
        logger.info("="*80)

        return {
            'status': 'success',
            'run_id': run_id,
            'total_ads_analyzed': len(manifest_data.get('ads', [])),
            'report_path': str(saved_files['report_path']),
            'report_filename': saved_files['report_filename'],
            'provider': 'openai',
            'model': 'gpt-3.5-turbo',
            'analysis_summary': {
                'generated_at': datetime.now().isoformat()
            },
            'full_analysis': analysis_json
        }

    except HTTPException:
        logger.error("\n‚ùå Error HTTP capturado, reenviando...")
        raise
    except Exception as e:
        logger.error(f"\n‚ùå ERROR INESPERADO: {str(e)}")
        logger.exception("Traceback completo:")
        raise HTTPException(
            status_code=500,
            detail=f"Error inesperado: {str(e)}"
        )


@router.post(
    '/analyze-from-run-id',
    tags=["ai-analysis"],
    summary="An√°lisis con OpenAI desde run_id existente"
)
async def analyze_from_run_id(run_id: str) -> Dict[str, Any]:
    """
    An√°lisis completo con OpenAI desde un run_id que ya tiene datos.

    Este endpoint:
    1. Toma un run_id de un scraping exitoso previo
    2. Analiza el dataset local
    3. Descarga multimedia de los top anuncios
    4. Sube archivos a GCS
    5. Construye manifest con URLs p√∫blicas
    6. Analiza con OpenAI GPT-4o Vision
    7. Guarda resultados en JSON

    Args:
        run_id: ID del run ya existente con datos scrapeados

    Returns:
        JSON con an√°lisis completo y manifest

    Tiempo estimado: 1-3 minutos
    """
    logger.info("="*80)
    logger.info("üöÄ AN√ÅLISIS DESDE RUN_ID EXISTENTE")
    logger.info("="*80)
    logger.info(f"üìã RUN_ID: {run_id}")

    try:
        # PASO 1: Verificar OpenAI
        logger.info("\nüì° PASO 1/7: Verificando configuraci√≥n OpenAI...")
        from openai import OpenAI

        api_key = os.getenv("OPEN_API_KEY")
        if not api_key:
            logger.error(f"   ‚ùå RUN_ID {run_id}: OPEN_API_KEY no configurada")
            raise HTTPException(
                status_code=503,
                detail="OPEN_API_KEY no configurada"
            )

        openai_client = OpenAI(api_key=api_key)
        logger.info(f"   ‚úÖ RUN_ID {run_id}: OpenAI configurado correctamente")

        # PASO 2: Verificar si ya existe en GCS
        logger.info("\n‚òÅÔ∏è  PASO 2/7: Verificando archivos en GCS...")
        gcs_service = get_gcs_service()
        if not gcs_service:
            logger.error(f"   ‚ùå RUN_ID {run_id}: GCS no configurado")
            raise HTTPException(503, "GCS no configurado")

        bucket_name = gcs_service.default_bucket_name
        gcs_prefix = f'runs/{run_id}/prepared/'

        logger.info(
            f"   üîç RUN_ID {run_id}: Buscando en gs://{bucket_name}/{gcs_prefix}")

        # Listar archivos en GCS
        from google.cloud import storage
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blobs = list(bucket.list_blobs(prefix=gcs_prefix))

        manifest = {'run_id': run_id, 'ads': []}
        already_in_gcs = False

        if blobs:
            logger.info(
                f"   ‚úÖ RUN_ID {run_id}: Encontrados {len(blobs)} archivos en GCS")
            logger.info(
                f"   ‚ÑπÔ∏è  RUN_ID {run_id}: Usando archivos existentes (sin re-procesar)")
            already_in_gcs = True

            # Construir manifest desde GCS
            media_by_ad = {}
            for blob in blobs:
                # Extraer ad_id: runs/{run_id}/prepared/{ad_id}/{filename}
                parts = blob.name.split('/')
                if len(parts) >= 4:
                    ad_id = parts[3]
                    if ad_id not in media_by_ad:
                        media_by_ad[ad_id] = []
                        logger.debug(
                            f"   üìÅ RUN_ID {run_id}: Ad ID {ad_id} detectado")

                    public_url = (
                        f"https://storage.googleapis.com/"
                        f"{bucket_name}/{blob.name}"
                    )
                    is_video = blob.name.endswith('.mp4')
                    file_type = 'video' if is_video else 'image'

                    media_by_ad[ad_id].append({
                        'url': public_url,
                        'type': file_type
                    })

            for ad_id, files in media_by_ad.items():
                manifest['ads'].append({
                    'ad_id': ad_id,
                    'files': files
                })

            logger.info(
                f"   ‚úÖ RUN_ID {run_id}: Manifest construido desde GCS "
                f"({len(manifest['ads'])} anuncios, "
                f"{sum(len(ad['files']) for ad in manifest['ads'])} archivos)"
            )
        else:
            logger.warning(
                f"   ‚ö†Ô∏è  RUN_ID {run_id}: No encontrado en GCS"
            )
            logger.info(
                f"   üîÑ RUN_ID {run_id}: Iniciando preparaci√≥n desde datos locales..."
            )

        # PASO 3: Si no est√° en GCS, procesar localmente
        if not already_in_gcs:
            logger.info("\nüìä PASO 3/7: Procesando dataset local...")
            from pathlib import Path
            from app.processors.facebook.analyze_dataset import (
                analyze, analyze_jsonl
            )

            base_dir = get_facebook_saved_base()
            run_dir = base_dir / run_id

            logger.info(f"   üîç RUN_ID {run_id}: Buscando en {run_dir}")

            if not run_dir.exists():
                logger.error(
                    f"   ‚ùå RUN_ID {run_id}: No encontrado en local ni en GCS"
                )
                logger.error(f"   üìÇ RUN_ID {run_id}: Ruta buscada: {run_dir}")
                raise HTTPException(
                    status_code=404,
                    detail=f"Run ID {run_id} no encontrado localmente ni en GCS"
                )

            logger.info(f"   ‚úÖ RUN_ID {run_id}: Directorio encontrado")

            csv_path = run_dir / f"{run_id}.csv"
            jsonl_path = run_dir / f"{run_id}.jsonl"

            stats = {}
            if csv_path.exists():
                logger.info(f"   üìÑ RUN_ID {run_id}: Procesando CSV...")
                stats = analyze(csv_path)
                logger.info(
                    f"   ‚úÖ RUN_ID {run_id}: CSV analizado "
                    f"({len(stats)} anuncios encontrados)"
                )
            elif jsonl_path.exists():
                logger.info(f"   üìÑ RUN_ID {run_id}: Procesando JSONL...")
                stats = analyze_jsonl(jsonl_path)
                logger.info(
                    f"   ‚úÖ RUN_ID {run_id}: JSONL analizado "
                    f"({len(stats)} anuncios encontrados)"
                )
            else:
                logger.error(
                    f"   ‚ùå RUN_ID {run_id}: No se encontr√≥ CSV ni JSONL"
                )
                logger.error(f"   üìÇ CSV esperado: {csv_path}")
                logger.error(f"   üìÇ JSONL esperado: {jsonl_path}")
                raise HTTPException(
                    status_code=404,
                    detail="No se encontr√≥ CSV ni JSONL en directorio local"
                )

            # Tomar top 10
            items = sorted(
                stats.values(),
                key=lambda x: x.get('score', 0),
                reverse=True
            )
            top_ads = [it.get('ad_id') for it in items[:10]]
            logger.info(
                f"   ‚úÖ RUN_ID {run_id}: Top 10 anuncios seleccionados "
                f"(IDs: {', '.join(map(str, top_ads[:3]))}...)"
            )

            # PASO 4: Descargar multimedia
            logger.info("\nüì• PASO 4/7: Descargando multimedia...")
            from app.processors.facebook.download_images_from_csv import (
                make_session,
                download_one,
                iter_csv_snapshot_rows,
                extract_urls_from_snapshot
            )
            from concurrent.futures import ThreadPoolExecutor, as_completed

            media_dir = run_dir / 'media'
            media_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"   üìÅ RUN_ID {run_id}: Directorio media: {media_dir}")

            session = make_session()

            downloaded = 0
            total_urls = 0
            if csv_path.exists():
                logger.info(
                    f"   üîÑ RUN_ID {run_id}: Extrayendo URLs de multimedia..."
                )
                with ThreadPoolExecutor(max_workers=6) as ex:
                    futures = []
                    for row, snapshot in iter_csv_snapshot_rows(csv_path):
                        ad_id = (
                            row.get('ad_archive_id') or
                            row.get('ad_id') or
                            'unknown'
                        )
                        if ad_id not in top_ads:
                            continue
                        if not snapshot:
                            continue
                        urls = extract_urls_from_snapshot(snapshot)
                        total_urls += len(urls[:5])
                        for u in urls[:5]:
                            futures.append(
                                ex.submit(
                                    download_one,
                                    session,
                                    u,
                                    media_dir,
                                    prefix=ad_id
                                )
                            )

                    logger.info(
                        f"   üåê RUN_ID {run_id}: Descargando {total_urls} archivos..."
                    )

                    for fut in as_completed(futures):
                        url, path = fut.result()
                        if path:
                            downloaded += 1
                            if downloaded % 10 == 0:
                                logger.debug(
                                    f"   üì• RUN_ID {run_id}: "
                                    f"Descargados {downloaded}/{total_urls}"
                                )

            logger.info(
                f"   ‚úÖ RUN_ID {run_id}: {downloaded} archivos descargados "
                f"de {total_urls} URLs procesadas"
            )

            # PASO 5: Preparar archivos
            logger.info("\nüìÇ PASO 5/7: Preparando archivos...")
            import shutil

            prepared_dir = run_dir / 'prepared'
            prepared_dir.mkdir(parents=True, exist_ok=True)
            logger.info(
                f"   üìÅ RUN_ID {run_id}: Directorio prepared: {prepared_dir}")

            files_prepared = 0
            for ad_id in top_ads:
                ad_dir = prepared_dir / str(ad_id)
                ad_dir.mkdir(parents=True, exist_ok=True)

                matched = [
                    p for p in media_dir.iterdir()
                    if p.is_file() and p.name.startswith(str(ad_id))
                ]

                logger.debug(
                    f"   üìã RUN_ID {run_id}: Ad {ad_id} - "
                    f"{len(matched)} archivos encontrados"
                )

                for p in matched[:5]:
                    shutil.copy2(p, ad_dir / p.name)
                    files_prepared += 1

            logger.info(
                f"   ‚úÖ RUN_ID {run_id}: {files_prepared} archivos preparados "
                f"({len(top_ads)} anuncios)"
            )

            # PASO 6: Subir a GCS
            logger.info("\n‚òÅÔ∏è  PASO 6/7: Subiendo a GCS...")
            logger.info(
                f"   üåê RUN_ID {run_id}: "
                f"Destino: gs://{bucket_name}/{gcs_prefix}"
            )

            uploaded_files = []
            total_files = sum(
                1 for _ in prepared_dir.rglob('*')
                if _.is_file()
            )
            logger.info(
                f"   üì§ RUN_ID {run_id}: Subiendo {total_files} archivos..."
            )

            for file_path in prepared_dir.rglob('*'):
                if not file_path.is_file():
                    continue

                relative_path = file_path.relative_to(prepared_dir)
                blob_name = f'{gcs_prefix}{relative_path}'.replace('\\', '/')

                logger.debug(
                    f"   üì§ RUN_ID {run_id}: Subiendo {file_path.name}"
                )

                result = gcs_service.upload_file(
                    local_path=str(file_path),
                    blob_name=blob_name,
                    bucket_name=bucket_name
                )
                uploaded_files.append({
                    'blob_name': blob_name,
                    'public_url': result['public_url'],
                    'ad_id': file_path.parent.name
                })

            logger.info(
                f"   ‚úÖ RUN_ID {run_id}: {len(uploaded_files)} archivos subidos "
                f"a GCS correctamente"
            )

            # Construir manifest desde archivos subidos
            media_by_ad = {}

            for file_info in uploaded_files:
                ad_id = file_info['ad_id']
                if ad_id not in media_by_ad:
                    media_by_ad[ad_id] = []

                is_video = file_info['blob_name'].endswith('.mp4')
                file_type = 'video' if is_video else 'image'
                media_by_ad[ad_id].append({
                    'url': file_info['public_url'],
                    'type': file_type
                })

            for ad_id, files in media_by_ad.items():
                manifest['ads'].append({
                    'ad_id': ad_id,
                    'files': files
                })

            logger.info(
                f"   ‚úÖ RUN_ID {run_id}: Manifest construido "
                f"({len(manifest['ads'])} anuncios, "
                f"{sum(len(ad['files']) for ad in manifest['ads'])} archivos)"
            )

        # Validar manifest
        if not manifest['ads']:
            logger.error(f"   ‚ùå RUN_ID {run_id}: Manifest vac√≠o, sin anuncios")
            raise HTTPException(
                status_code=400,
                detail="No hay anuncios en el manifest"
            )

        # PASO 7: Analizar con OpenAI
        logger.info("\nü§ñ PASO 7/7: Analizando con OpenAI GPT-4o...")
        logger.info(
            f"   üìä RUN_ID {run_id}: Enviando {len(manifest['ads'])} anuncios "
            f"con {sum(len(ad['files']) for ad in manifest['ads'])} im√°genes"
        )

        # Cargar prompt con fallback chain: .env PROMPT ‚Üí prompt.txt ‚Üí DEFAULT_PROMPT
        env_prompt = os.getenv('PROMPT')
        if not env_prompt:
            prompt_file_name = os.getenv('PROMPT_FILE', 'prompt.txt')
            # 7 niveles para llegar a api_service/ desde analysis.py
            api_service_dir = Path(
                __file__).parent.parent.parent.parent.parent.parent.parent
            prompt_file_path = api_service_dir / prompt_file_name

            logger.info(f"   üîç Buscando prompt en: {prompt_file_path}")

            if prompt_file_path.exists():
                env_prompt = prompt_file_path.read_text(encoding='utf-8')
                logger.info(
                    f"   ‚úÖ RUN_ID {run_id}: Prompt cargado desde {prompt_file_name} "
                    f"({len(env_prompt)} caracteres)")
            else:
                logger.warning(
                    f"   ‚ö†Ô∏è RUN_ID {run_id}: Archivo {prompt_file_name} no encontrado en {api_service_dir}, "
                    f"usando DEFAULT_PROMPT")
                env_prompt = DEFAULT_PROMPT
        else:
            logger.info(
                f"   üìù RUN_ID {run_id}: Usando prompt del .env ({len(env_prompt)} chars)")

        logger.info(
            f"   üìù RUN_ID {run_id}: Prompt final cargado ({len(env_prompt)} chars)")

        # Construir mensaje con im√°genes
        content = [
            {
                "type": "text",
                "text": f"{env_prompt}\n\nAnaliza los siguientes anuncios:"
            }
        ]

        image_count = 0
        for ad in manifest['ads']:
            ad_id = ad.get('ad_id', 'N/A')
            content.append({
                "type": "text",
                "text": f"\n--- Anuncio ID: {ad_id} ---"
            })

            for file_info in ad.get('files', []):
                if file_info.get('type') == 'image':
                    content.append({
                        "type": "image_url",
                        "image_url": {"url": file_info['url']}
                    })
                    image_count += 1

        logger.info(
            f"   üñºÔ∏è  RUN_ID {run_id}: Preparadas {image_count} im√°genes "
            f"para an√°lisis"
        )

        messages = [{"role": "user", "content": content}]

        logger.info(f"   üöÄ RUN_ID {run_id}: Enviando solicitud a OpenAI...")

        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=4096,
            temperature=0.5  # Balance precisi√≥n-creatividad para an√°lisis
        )

        logger.info(f"   ‚úÖ RUN_ID {run_id}: Respuesta recibida de OpenAI")

        analysis_text = response.choices[0].message.content or ""
        tokens_used = response.usage.total_tokens if response.usage else 0

        logger.info(
            f"   üìä RUN_ID {run_id}: Tokens usados: {tokens_used} "
            f"(an√°lisis: {len(analysis_text)} chars)"
        )

        analysis_json = {
            "run_id": run_id,
            "analysis": analysis_text,
            "model": "gpt-4o",
            "ads_count": len(manifest['ads']),
            "tokens_used": tokens_used,
            "manifest": manifest
        }

        logger.info(f"   ‚úÖ RUN_ID {run_id}: An√°lisis completado exitosamente")

        # Guardar resultados
        logger.info(f"   üíæ RUN_ID {run_id}: Guardando reporte...")

        save_base_dir = get_facebook_saved_base()
        reports_dir = save_base_dir / 'reports_json'
        reports_dir.mkdir(parents=True, exist_ok=True)

        report_filename = f"{run_id}_analysis.json"
        report_path = reports_dir / report_filename

        import json
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_json, f, ensure_ascii=False, indent=2)

        logger.info(
            f"   ‚úÖ RUN_ID {run_id}: Reporte guardado en {report_filename}")
        logger.info("="*80)
        logger.info(f"üéâ RUN_ID {run_id}: AN√ÅLISIS COMPLETADO EXITOSAMENTE")
        logger.info("="*80)

        return {
            'status': 'success',
            'run_id': run_id,
            'total_ads_analyzed': len(manifest['ads']),
            'report_path': str(report_path),
            'report_filename': report_filename,
            'provider': 'openai',
            'model': 'gpt-4o',
            'analysis_summary': {
                'generated_at': datetime.now().isoformat()
            },
            'full_analysis': analysis_json
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error("="*80)
        logger.error(f"‚ùå RUN_ID {run_id}: ERROR EN AN√ÅLISIS")
        logger.error("="*80)
        logger.error(
            f"   üî¥ RUN_ID {run_id}: Tipo de error: {type(e).__name__}")
        logger.error(f"   üî¥ RUN_ID {run_id}: Mensaje: {str(e)}")
        logger.exception(f"   üìã RUN_ID {run_id}: Traceback completo:")
        raise HTTPException(
            status_code=500,
            detail=f"Error en an√°lisis de RUN_ID {run_id}: {str(e)}"
        )


@router.post(
    '/analyze-local-only',
    tags=["ai-analysis"],
    summary="An√°lisis con OpenAI solo con archivos locales (sin GCS)"
)
async def analyze_local_only(
    run_id: str,
    top_n: int = 10
) -> Dict[str, Any]:
    """
    An√°lisis completo con OpenAI usando SOLO archivos locales.
    NO requiere acceso a GCS.

    Este endpoint:
    1. Lee el dataset local (CSV/JSONL)
    2. Analiza y selecciona top N anuncios
    3. Descarga multimedia localmente
    4. Codifica im√°genes en base64
    5. Env√≠a a OpenAI para an√°lisis
    6. Retorna JSON con an√°lisis completo

    Args:
        run_id: ID del run con datos locales
        top_n: N√∫mero de anuncios a analizar (default: 10)

    Returns:
        JSON con an√°lisis completo

    NO requiere: GCS, credenciales cloud, acceso a bucket
    """
    logger.info("="*80)
    logger.info("üöÄ AN√ÅLISIS LOCAL (SIN GCS)")
    logger.info("="*80)
    logger.info(f"üìã RUN_ID: {run_id}")
    logger.info(f"üìä Top N: {top_n}")

    try:
        # PASO 1: Verificar OpenAI
        logger.info("\nüì° PASO 1/5: Verificando OpenAI...")
        from openai import OpenAI

        api_key = os.getenv("OPEN_API_KEY")
        if not api_key:
            logger.error(f"   ‚ùå RUN_ID {run_id}: OPEN_API_KEY no configurada")
            raise HTTPException(503, "OPEN_API_KEY no configurada")

        openai_client = OpenAI(api_key=api_key)
        logger.info(f"   ‚úÖ RUN_ID {run_id}: OpenAI configurado")

        # PASO 2: Leer dataset local
        logger.info("\nüìä PASO 2/5: Procesando dataset local...")
        from pathlib import Path
        from app.processors.facebook.analyze_dataset import (
            analyze, analyze_jsonl
        )

        base_dir = get_facebook_saved_base()
        run_dir = base_dir / run_id

        logger.info(f"   üîç RUN_ID {run_id}: Buscando en {run_dir}")

        if not run_dir.exists():
            logger.error(f"   ‚ùå RUN_ID {run_id}: Directorio no encontrado")
            raise HTTPException(404, f"Run ID {run_id} no encontrado")

        csv_path = run_dir / f"{run_id}.csv"
        jsonl_path = run_dir / f"{run_id}.jsonl"

        stats = {}
        if csv_path.exists():
            logger.info(f"   üìÑ RUN_ID {run_id}: Procesando CSV...")
            stats = analyze(csv_path)
            logger.info(f"   ‚úÖ RUN_ID {run_id}: {len(stats)} anuncios")
        elif jsonl_path.exists():
            logger.info(f"   üìÑ RUN_ID {run_id}: Procesando JSONL...")
            stats = analyze_jsonl(jsonl_path)
            logger.info(f"   ‚úÖ RUN_ID {run_id}: {len(stats)} anuncios")
        else:
            logger.error(f"   ‚ùå RUN_ID {run_id}: No se encontr√≥ dataset")
            raise HTTPException(404, "No se encontr√≥ CSV ni JSONL")

        # Seleccionar top N
        items = sorted(
            stats.values(),
            key=lambda x: x.get('score', 0),
            reverse=True
        )
        top_ads = [it.get('ad_id') for it in items[:top_n]]
        logger.info(
            f"   ‚úÖ RUN_ID {run_id}: Top {len(top_ads)} seleccionados"
        )

        # PASO 3: Descargar multimedia
        logger.info("\nüì• PASO 3/5: Descargando multimedia...")
        from app.processors.facebook.download_images_from_csv import (
            make_session,
            download_one,
            iter_csv_snapshot_rows,
            extract_urls_from_snapshot
        )
        from concurrent.futures import ThreadPoolExecutor, as_completed

        media_dir = run_dir / 'media'
        media_dir.mkdir(parents=True, exist_ok=True)
        session = make_session()

        downloaded = 0
        if csv_path.exists():
            with ThreadPoolExecutor(max_workers=6) as ex:
                futures = []
                for row, snapshot in iter_csv_snapshot_rows(csv_path):
                    ad_id = (
                        row.get('ad_archive_id') or
                        row.get('ad_id') or
                        'unknown'
                    )
                    if ad_id not in top_ads:
                        continue
                    if not snapshot:
                        continue
                    urls = extract_urls_from_snapshot(snapshot)
                    for u in urls[:5]:
                        futures.append(
                            ex.submit(
                                download_one,
                                session,
                                u,
                                media_dir,
                                prefix=ad_id
                            )
                        )

                for fut in as_completed(futures):
                    url, path = fut.result()
                    if path:
                        downloaded += 1

        logger.info(f"   ‚úÖ RUN_ID {run_id}: {downloaded} archivos descargados")

        # PASO 4: Codificar multimedia (im√°genes + videos)
        logger.info(
            "\nüñºÔ∏è  PASO 4/5: Procesando multimedia (im√°genes + videos)...")
        import base64
        from app.utils.video_utils import extract_frames_from_video

        ads_with_media = []
        total_images = 0
        total_video_frames = 0
        total_videos = 0

        # Primero: detectar TODOS los videos disponibles
        all_videos = [
            p for p in media_dir.iterdir()
            if p.is_file() and p.suffix.lower() in ['.mp4', '.mov', '.avi', '.webm']
        ]
        logger.info(f"   üìπ Detectados {len(all_videos)} videos en total")
        if all_videos:
            for v in all_videos[:3]:  # Mostrar primeros 3
                logger.info(f"      - {v.name}")

        for ad_id in top_ads:
            # Buscar archivos del anuncio (con patr√≥n m√°s flexible)
            ad_id_str = str(ad_id)
            matched = [
                p for p in media_dir.iterdir()
                if p.is_file() and (
                    p.name.startswith(ad_id_str + '_') or
                    p.name.startswith(ad_id_str + '.')
                )
            ]

            logger.info(
                f"   üîç AD {ad_id}: {len(matched)} archivos encontrados")

            media_items = []

            # Procesar multimedia (max 3 archivos por anuncio para velocidad)
            for media_path in matched[:3]:
                file_ext = media_path.suffix.lower()

                # IM√ÅGENES
                if file_ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                    try:
                        with open(media_path, 'rb') as f:
                            img_data = f.read()
                            b64_str = base64.b64encode(
                                img_data).decode('utf-8')

                            mime_type = 'image/jpeg'
                            if file_ext == '.png':
                                mime_type = 'image/png'
                            elif file_ext == '.gif':
                                mime_type = 'image/gif'
                            elif file_ext == '.webp':
                                mime_type = 'image/webp'

                            media_items.append({
                                'type': 'image',
                                'filename': media_path.name,
                                'mime_type': mime_type,
                                'base64': b64_str
                            })
                            total_images += 1
                    except Exception as e:
                        logger.warning(
                            f"   ‚ö†Ô∏è  Error con imagen {media_path.name}: {e}")

                # VIDEOS: Extraer 5 frames
                elif file_ext in ['.mp4', '.mov', '.avi', '.webm']:
                    try:
                        logger.info(
                            f"   üé• Extrayendo frames de {media_path.name}...")
                        frames = extract_frames_from_video(
                            media_path, num_frames=5)

                        if frames:
                            for frame in frames:
                                media_items.append({
                                    'type': 'video_frame',
                                    'source_video': media_path.name,
                                    'frame_number': frame['frame_number'],
                                    'timestamp': frame['timestamp'],
                                    'mime_type': 'image/jpeg',
                                    'base64': frame['base64']
                                })
                                total_video_frames += 1

                            total_videos += 1
                            logger.info(
                                f"      ‚úÖ {len(frames)} frames extra√≠dos")
                    except Exception as e:
                        logger.warning(
                            f"   ‚ö†Ô∏è  Error con video {media_path.name}: {e}")

            if media_items:
                ads_with_media.append({
                    'ad_id': ad_id,
                    'media': media_items
                })

        # Si no se procesaron videos pero hay videos disponibles, procesar los primeros
        if total_videos == 0 and all_videos:
            logger.info(
                f"   ‚ö†Ô∏è  Videos no asociados a top_ads, procesando {min(2, len(all_videos))} videos...")
            for video_path in all_videos[:2]:
                try:
                    logger.info(
                        f"   üé• Extrayendo frames de {video_path.name}...")
                    frames = extract_frames_from_video(
                        video_path, num_frames=5)

                    if frames:
                        video_items = []
                        for frame in frames:
                            video_items.append({
                                'type': 'video_frame',
                                'source_video': video_path.name,
                                'frame_number': frame['frame_number'],
                                'timestamp': frame['timestamp'],
                                'mime_type': 'image/jpeg',
                                'base64': frame['base64']
                            })
                            total_video_frames += 1

                        total_videos += 1
                        logger.info(f"      ‚úÖ {len(frames)} frames extra√≠dos")

                        # Agregar como anuncio "extra"
                        ads_with_media.append({
                            'ad_id': f'video_{video_path.stem}',
                            'media': video_items
                        })
                except Exception as e:
                    logger.warning(
                        f"   ‚ö†Ô∏è  Error con video {video_path.name}: {e}")

        logger.info(
            f"   ‚úÖ RUN_ID {run_id}: {total_images} im√°genes + "
            f"{total_video_frames} frames ({total_videos} videos) - "
            f"{len(ads_with_media)} anuncios"
        )

        if not ads_with_media:
            logger.error(f"   ‚ùå RUN_ID {run_id}: No hay multimedia")
            raise HTTPException(400, "No se encontr√≥ multimedia para analizar")

        # PASO 5: Analizar con OpenAI
        logger.info("\nü§ñ PASO 5/5: Analizando con OpenAI...")

        # Cargar prompt con fallback chain: .env PROMPT ‚Üí prompt.txt ‚Üí DEFAULT_PROMPT
        env_prompt = os.getenv('PROMPT')
        if not env_prompt:
            prompt_file_name = os.getenv('PROMPT_FILE', 'prompt.txt')
            # 7 niveles para llegar a api_service/ desde analysis.py
            api_service_dir = Path(
                __file__).parent.parent.parent.parent.parent.parent.parent
            prompt_file_path = api_service_dir / prompt_file_name

            logger.info(f"   üîç Buscando prompt en: {prompt_file_path}")

            if prompt_file_path.exists():
                env_prompt = prompt_file_path.read_text(encoding='utf-8')
                logger.info(
                    f"   ‚úÖ RUN_ID {run_id}: Prompt cargado desde {prompt_file_name} "
                    f"({len(env_prompt)} caracteres)")
            else:
                logger.warning(
                    f"   ‚ö†Ô∏è RUN_ID {run_id}: Archivo {prompt_file_name} no encontrado en {api_service_dir}, "
                    f"usando DEFAULT_PROMPT")
                env_prompt = DEFAULT_PROMPT
        else:
            logger.info(
                f"   üìù RUN_ID {run_id}: Usando prompt del .env ({len(env_prompt)} chars)")

        # Preparar metadatos del CSV para incluir en el prompt
        import csv
        import json as json_module

        csv_metadata = []
        if csv_path.exists():
            logger.info(
                f"   ÔøΩ RUN_ID {run_id}: Extrayendo metadatos del CSV...")
            with open(csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    ad_id = row.get('ad_archive_id') or row.get(
                        'adArchiveID') or row.get('id')
                    if ad_id in [ad['ad_id'] for ad in ads_with_media]:
                        # Incluir solo campos relevantes
                        metadata = {
                            'ad_id': ad_id,
                            'page_name': row.get('page_name', 'N/A'),
                            'ad_creation_time': row.get('ad_creation_time', 'N/A'),
                            'ad_creative_bodies': row.get('ad_creative_bodies', 'N/A'),
                            'ad_creative_link_titles': row.get('ad_creative_link_titles', 'N/A'),
                            'ad_creative_link_descriptions': row.get('ad_creative_link_descriptions', 'N/A'),
                            'impressionsMin': row.get('impressionsMin', 'N/A'),
                            'impressionsMax': row.get('impressionsMax', 'N/A'),
                            'spendMin': row.get('spendMin', 'N/A'),
                            'spendMax': row.get('spendMax', 'N/A'),
                        }
                        csv_metadata.append(metadata)

            logger.info(
                f"   ‚úÖ RUN_ID {run_id}: {len(csv_metadata)} registros de metadatos extra√≠dos")

        # Construir mensaje con prompt + metadatos del CSV + multimedia
        metadata_text = ""
        if csv_metadata:
            metadata_text = "\n\n**DATOS DE ENTRADA - Metadatos del CSV:**\n```json\n"
            metadata_text += json_module.dumps(csv_metadata,
                                               ensure_ascii=False, indent=2)
            metadata_text += "\n```\n"
            logger.info(
                f"   üìä Metadatos CSV incluidos: {len(csv_metadata)} anuncios")

        # Construir texto completo del prompt
        full_prompt_text = env_prompt + metadata_text

        logger.info(
            f"   üìù Longitud prompt completo: {len(full_prompt_text)} caracteres")
        logger.info(
            f"   üìù Primeros 200 chars del prompt: {env_prompt[:200]}...")

        content = [
            {
                "type": "text",
                "text": full_prompt_text
            }
        ]

        # Agregar im√°genes y frames de videos de cada anuncio
        media_count = 0
        for ad in ads_with_media:
            ad_id = ad.get('ad_id', 'unknown')
            for media in ad['media']:
                content.append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:{media['mime_type']};base64,{media['base64']}"
                    }
                })
                media_count += 1

        logger.info(
            f"   üñºÔ∏è  Total elementos multimedia agregados: {media_count}")

        logger.info(
            f"   üöÄ RUN_ID {run_id}: Enviando a GPT-4o-mini "
            f"({len(ads_with_media)} anuncios, {total_images} im√°genes, "
            f"{total_video_frames} frames de video)..."
        )

        messages = [{"role": "user", "content": content}]

        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_tokens=4096,
            temperature=0.5  # Balance precisi√≥n-creatividad para an√°lisis
        )

        logger.info(f"   ‚úÖ RUN_ID {run_id}: Respuesta recibida")

        analysis_text = response.choices[0].message.content or ""
        tokens_used = response.usage.total_tokens if response.usage else 0

        logger.info(
            f"   üìä RUN_ID {run_id}: Tokens: {tokens_used}, "
            f"Chars: {len(analysis_text)}"
        )

        # Construir respuesta JSON
        result = {
            'status': 'success',
            'run_id': run_id,
            'mode': 'local-only',
            'total_ads_analyzed': len(ads_with_media),
            'total_images': total_images,
            'total_video_frames': total_video_frames,
            'total_videos': total_videos,
            'tokens_used': tokens_used,
            'model': 'gpt-4o-mini',
            'analysis': analysis_text,
            'timestamp': datetime.now().isoformat(),
            'ads_details': [
                {
                    'ad_id': ad['ad_id'],
                    'media_count': len(ad['media']),
                    'images': sum(1 for m in ad['media'] if m['type'] == 'image'),
                    'video_frames': sum(1 for m in ad['media'] if m['type'] == 'video_frame')
                }
                for ad in ads_with_media
            ]
        }

        # Guardar JSON local
        logger.info(f"   üíæ RUN_ID {run_id}: Guardando resultado...")
        reports_dir = base_dir / 'reports_json'
        reports_dir.mkdir(parents=True, exist_ok=True)

        report_filename = f"{run_id}_local_analysis.json"
        report_path = reports_dir / report_filename

        import json
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        logger.info(f"   ‚úÖ RUN_ID {run_id}: Guardado en {report_filename}")
        logger.info("="*80)
        logger.info(f"üéâ RUN_ID {run_id}: AN√ÅLISIS LOCAL COMPLETADO")
        logger.info("="*80)

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error("="*80)
        logger.error(f"‚ùå RUN_ID {run_id}: ERROR EN AN√ÅLISIS LOCAL")
        logger.error("="*80)
        logger.error(f"   üî¥ Tipo: {type(e).__name__}")
        logger.error(f"   üî¥ Mensaje: {str(e)}")
        logger.exception("   üìã Traceback:")
        raise HTTPException(500, f"Error: {str(e)}")


@router.post(
    '/compare-campaigns',
    tags=["ai-analysis"],
    summary="Comparaci√≥n de 2 campa√±as con an√°lisis detallado de multimedia"
)
async def compare_campaigns(
    run_id_1: str,
    run_id_2: str,
    top_n: int = 10
) -> Dict[str, Any]:
    """
    An√°lisis comparativo de 2 campa√±as con OpenAI.

    Compara:
    - Rendimiento de im√°genes vs videos
    - M√©tricas de ambas campa√±as
    - Informaci√≥n detallada de cada archivo multimedia
    - An√°lisis visual con OpenAI GPT-4o

    Args:
        run_id_1: ID de la primera campa√±a
        run_id_2: ID de la segunda campa√±a
        top_n: Anuncios a analizar por campa√±a (default: 10)

    Returns:
        JSON con an√°lisis comparativo completo
    """
    logger.info("="*80)
    logger.info("üîÄ AN√ÅLISIS COMPARATIVO DE CAMPA√ëAS")
    logger.info("="*80)
    logger.info(f"üìã Campa√±a 1: {run_id_1}")
    logger.info(f"üìã Campa√±a 2: {run_id_2}")
    logger.info(f"üìä Top N por campa√±a: {top_n}")

    try:
        # PASO 1: Verificar OpenAI
        logger.info("\nüì° PASO 1/6: Verificando OpenAI...")
        from openai import OpenAI

        api_key = os.getenv("OPEN_API_KEY")
        if not api_key:
            raise HTTPException(503, "OPEN_API_KEY no configurada")

        openai_client = OpenAI(api_key=api_key)
        logger.info("   ‚úÖ OpenAI configurado")

        # PASO 2: Procesar ambas campa√±as
        logger.info("\nüìä PASO 2/6: Procesando ambas campa√±as...")
        from pathlib import Path
        import csv
        import json as json_module
        from app.processors.facebook.analyze_dataset import analyze

        base_dir = get_facebook_saved_base()

        campaigns_data = []

        for idx, run_id in enumerate([run_id_1, run_id_2], 1):
            logger.info(f"\n   üìÇ Procesando Campa√±a {idx}: {run_id}")

            run_dir = base_dir / run_id
            if not run_dir.exists():
                raise HTTPException(404, f"Run ID {run_id} no encontrado")

            csv_path = run_dir / f"{run_id}.csv"
            if not csv_path.exists():
                raise HTTPException(404, f"CSV no encontrado para {run_id}")

            # Analizar dataset
            stats = analyze(csv_path)
            items = sorted(
                stats.values(),
                key=lambda x: x.get('score', 0),
                reverse=True
            )
            top_ads = [it.get('ad_id') for it in items[:top_n]]

            logger.info(
                f"      ‚úÖ {len(stats)} anuncios, top {len(top_ads)} seleccionados")

            # Extraer metadatos del CSV
            csv_metadata = []
            with open(csv_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    ad_id = (
                        row.get('ad_archive_id') or
                        row.get('adArchiveID') or
                        row.get('id')
                    )
                    if ad_id in top_ads:
                        csv_metadata.append({
                            'ad_id': ad_id,
                            'page_name': row.get('page_name', 'N/A'),
                            'ad_creation_time': row.get('ad_creation_time', 'N/A'),
                            'ad_creative_bodies': row.get('ad_creative_bodies', 'N/A'),
                            'impressionsMin': row.get('impressionsMin', 'N/A'),
                            'impressionsMax': row.get('impressionsMax', 'N/A'),
                            'spendMin': row.get('spendMin', 'N/A'),
                            'spendMax': row.get('spendMax', 'N/A'),
                        })

            campaigns_data.append({
                'run_id': run_id,
                'run_dir': run_dir,
                'top_ads': top_ads,
                'metadata': csv_metadata
            })

        # PASO 3: Descargar y analizar multimedia
        logger.info("\nüì• PASO 3/6: Descargando multimedia de ambas campa√±as...")
        from app.processors.facebook.download_images_from_csv import (
            make_session,
            download_one,
            iter_csv_snapshot_rows,
            extract_urls_from_snapshot
        )
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import base64

        session = make_session()

        for campaign in campaigns_data:
            run_id = campaign['run_id']
            run_dir = campaign['run_dir']
            csv_path = run_dir / f"{run_id}.csv"
            media_dir = run_dir / 'media'
            media_dir.mkdir(parents=True, exist_ok=True)

            logger.info(f"   üì• Campa√±a {run_id}: Descargando multimedia...")

            downloaded = 0
            with ThreadPoolExecutor(max_workers=6) as ex:
                futures = []
                for row, snapshot in iter_csv_snapshot_rows(csv_path):
                    ad_id = (
                        row.get('ad_archive_id') or
                        row.get('ad_id') or
                        'unknown'
                    )
                    if ad_id not in campaign['top_ads']:
                        continue
                    if not snapshot:
                        continue
                    urls = extract_urls_from_snapshot(snapshot)
                    for u in urls[:10]:  # Max 10 archivos por anuncio
                        futures.append(
                            ex.submit(
                                download_one,
                                session,
                                u,
                                media_dir,
                                prefix=ad_id
                            )
                        )

                for fut in as_completed(futures):
                    url, path = fut.result()
                    if path:
                        downloaded += 1

            logger.info(f"      ‚úÖ {downloaded} archivos descargados")
            campaign['downloaded_count'] = downloaded

        # PASO 4: Codificar multimedia y clasificar (imagen/video)
        logger.info("\nüñºÔ∏è  PASO 4/6: Codificando y clasificando multimedia...")

        for campaign in campaigns_data:
            run_id = campaign['run_id']
            media_dir = campaign['run_dir'] / 'media'

            logger.info(f"   üîÑ Campa√±a {run_id}: Procesando archivos...")

            media_files = []
            images_count = 0
            videos_count = 0

            for ad_id in campaign['top_ads']:
                matched = [
                    p for p in media_dir.iterdir()
                    if p.is_file() and p.name.startswith(str(ad_id))
                ]

                for file_path in matched[:10]:
                    file_ext = file_path.suffix.lower()
                    file_size = file_path.stat().st_size

                    # Clasificar tipo de archivo
                    if file_ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                        file_type = 'image'
                        images_count += 1

                        # Codificar imagen en base64
                        try:
                            with open(file_path, 'rb') as f:
                                img_data = f.read()
                                b64_str = base64.b64encode(
                                    img_data).decode('utf-8')

                                mime_type = 'image/jpeg'
                                if file_ext == '.png':
                                    mime_type = 'image/png'
                                elif file_ext == '.gif':
                                    mime_type = 'image/gif'
                                elif file_ext == '.webp':
                                    mime_type = 'image/webp'

                                media_files.append({
                                    'ad_id': ad_id,
                                    'filename': file_path.name,
                                    'type': file_type,
                                    'extension': file_ext,
                                    'size_bytes': file_size,
                                    'size_mb': round(file_size / (1024*1024), 2),
                                    'mime_type': mime_type,
                                    'base64': b64_str
                                })
                        except Exception as e:
                            logger.warning(
                                f"      ‚ö†Ô∏è  Error con {file_path.name}: {e}")

                    elif file_ext in ['.mp4', '.mov', '.avi', '.webm']:
                        file_type = 'video'
                        videos_count += 1

                        # Para videos, guardar metadata pero NO codificar
                        media_files.append({
                            'ad_id': ad_id,
                            'filename': file_path.name,
                            'type': file_type,
                            'extension': file_ext,
                            'size_bytes': file_size,
                            'size_mb': round(file_size / (1024*1024), 2),
                            'local_path': str(file_path),
                            'base64': None  # Videos no se env√≠an a OpenAI
                        })

            campaign['media_files'] = media_files
            campaign['images_count'] = images_count
            campaign['videos_count'] = videos_count

            logger.info(
                f"      ‚úÖ {images_count} im√°genes, {videos_count} videos"
            )

        # PASO 5: Preparar datos comparativos
        logger.info("\nüìä PASO 5/6: Preparando an√°lisis comparativo...")

        # Cargar prompt
        env_prompt = os.getenv('PROMPT')
        if not env_prompt:
            prompt_file_name = os.getenv('PROMPT_FILE', 'prompt.txt')
            prompt_file_path = (
                Path(__file__).parent.parent.parent.parent.parent.parent /
                prompt_file_name
            )
            if prompt_file_path.exists():
                env_prompt = prompt_file_path.read_text(encoding='utf-8')
            else:
                env_prompt = DEFAULT_PROMPT

        # Construir texto comparativo con metadatos
        comparison_text = "\n\n**AN√ÅLISIS COMPARATIVO DE 2 CAMPA√ëAS**\n\n"

        for idx, campaign in enumerate(campaigns_data, 1):
            comparison_text += f"**CAMPA√ëA {idx} (RUN_ID: {campaign['run_id']})**\n\n"
            comparison_text += f"- Total anuncios analizados: {len(campaign['top_ads'])}\n"
            comparison_text += f"- Archivos multimedia: {len(campaign['media_files'])}\n"
            comparison_text += f"  - Im√°genes: {campaign['images_count']}\n"
            comparison_text += f"  - Videos: {campaign['videos_count']}\n\n"

            comparison_text += "**Metadatos del CSV:**\n```json\n"
            comparison_text += json_module.dumps(
                campaign['metadata'],
                ensure_ascii=False,
                indent=2
            )
            comparison_text += "\n```\n\n"

            comparison_text += "**Detalle de archivos multimedia:**\n```json\n"
            # Info de archivos sin base64 (muy largo)
            files_info = [
                {
                    'ad_id': f['ad_id'],
                    'filename': f['filename'],
                    'type': f['type'],
                    'extension': f['extension'],
                    'size_mb': f['size_mb']
                }
                for f in campaign['media_files']
            ]
            comparison_text += json_module.dumps(
                files_info,
                ensure_ascii=False,
                indent=2
            )
            comparison_text += "\n```\n\n"

        # Construir mensaje con prompt + comparaci√≥n + solo im√°genes
        content = [
            {
                "type": "text",
                "text": env_prompt + comparison_text
            }
        ]

        # Agregar SOLO im√°genes (videos no soportados en base64)
        total_images_sent = 0
        for campaign in campaigns_data:
            for media in campaign['media_files']:
                if media['type'] == 'image' and media.get('base64'):
                    content.append({
                        "type": "image_url",
                        "image_url": {
                            "url": (
                                f"data:{media['mime_type']};"
                                f"base64,{media['base64']}"
                            )
                        }
                    })
                    total_images_sent += 1

        logger.info(
            f"   üì§ Enviando {total_images_sent} im√°genes a OpenAI "
            f"(videos solo como metadata)"
        )

        # PASO 6: Analizar con OpenAI
        logger.info("\nü§ñ PASO 6/6: Analizando con OpenAI...")

        messages = [{"role": "user", "content": content}]

        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=4096,
            temperature=0.5  # Balance precisi√≥n-creatividad para an√°lisis comparativo
        )

        analysis_text = response.choices[0].message.content or ""
        tokens_used = response.usage.total_tokens if response.usage else 0

        logger.info(
            f"   ‚úÖ An√°lisis completado ({tokens_used} tokens, "
            f"{len(analysis_text)} chars)"
        )

        # Construir respuesta
        result = {
            'status': 'success',
            'mode': 'campaign-comparison',
            'campaigns': [
                {
                    'run_id': c['run_id'],
                    'ads_analyzed': len(c['top_ads']),
                    'total_media_files': len(c['media_files']),
                    'images_count': c['images_count'],
                    'videos_count': c['videos_count'],
                    'metadata': c['metadata'],
                    'media_details': [
                        {
                            'ad_id': m['ad_id'],
                            'filename': m['filename'],
                            'type': m['type'],
                            'size_mb': m['size_mb']
                        }
                        for m in c['media_files']
                    ]
                }
                for c in campaigns_data
            ],
            'tokens_used': tokens_used,
            'model': 'gpt-4o',
            'analysis': analysis_text,
            'timestamp': datetime.now().isoformat()
        }

        # Guardar reporte
        logger.info("   üíæ Guardando reporte comparativo...")
        reports_dir = base_dir / 'reports_json'
        reports_dir.mkdir(parents=True, exist_ok=True)

        report_filename = f"comparison_{run_id_1}_vs_{run_id_2}.json"
        report_path = reports_dir / report_filename

        with open(report_path, 'w', encoding='utf-8') as f:
            json_module.dump(result, f, ensure_ascii=False, indent=2)

        logger.info(f"   ‚úÖ Guardado en {report_filename}")
        logger.info("="*80)
        logger.info("üéâ AN√ÅLISIS COMPARATIVO COMPLETADO")
        logger.info("="*80)

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error("="*80)
        logger.error("‚ùå ERROR EN AN√ÅLISIS COMPARATIVO")
        logger.error("="*80)
        logger.error(f"   üî¥ Tipo: {type(e).__name__}")
        logger.error(f"   üî¥ Mensaje: {str(e)}")
        logger.exception("   üìã Traceback:")
        raise HTTPException(500, f"Error: {str(e)}")


@router.post(
    '/generate-latex-report',
    tags=["ai-analysis"],
    summary="Genera reporte LaTeX desde an√°lisis JSON"
)
async def generate_latex_report(run_id: str) -> Dict[str, Any]:
    """
    Genera c√≥digo LaTeX profesional desde un an√°lisis JSON ya guardado.

    Este endpoint:
    1. Lee el archivo JSON de an√°lisis del run_id
    2. Usa OpenAI con prompt_latex.txt para generar LaTeX
    3. Guarda el archivo .tex
    4. Devuelve el c√≥digo LaTeX completo

    El usuario puede compilar el .tex con:
    - MikTeX (Windows)
    - TeX Live (Linux/Mac)
    - Overleaf (online)

    Args:
        run_id: ID de la campa√±a con an√°lisis JSON guardado

    Returns:
        JSON con c√≥digo LaTeX y ruta del archivo
    """
    logger.info("="*80)
    logger.info("üìÑ GENERACI√ìN DE REPORTE LATEX")
    logger.info("="*80)
    logger.info(f"üìã Run ID: {run_id}")

    try:
        # PASO 1: Verificar OpenAI
        logger.info("\nüì° PASO 1/4: Verificando OpenAI...")
        from openai import OpenAI

        api_key = os.getenv("OPEN_API_KEY")
        if not api_key:
            raise HTTPException(503, "OPEN_API_KEY no configurada")

        openai_client = OpenAI(api_key=api_key)
        logger.info("   ‚úÖ OpenAI configurado")

        # PASO 2: Leer an√°lisis JSON guardado
        logger.info("\nüìÇ PASO 2/4: Buscando an√°lisis JSON...")
        from pathlib import Path
        import json as json_module

        base_dir = get_facebook_saved_base()
        reports_dir = base_dir / 'reports_json'

        # Buscar archivo JSON del an√°lisis
        json_path = reports_dir / f"{run_id}_local_analysis.json"

        if not json_path.exists():
            raise HTTPException(
                404,
                f"No se encontr√≥ an√°lisis JSON para run_id: {run_id}. "
                f"Ejecuta primero /analyze-local-only"
            )

        logger.info(f"   ‚úÖ JSON encontrado: {json_path.name}")

        # Leer JSON
        with open(json_path, 'r', encoding='utf-8') as f:
            analysis_json = json_module.load(f)

        logger.info(
            f"   üìä Anuncios analizados: {analysis_json.get('total_ads_analyzed', 0)}")
        logger.info(f"   üñºÔ∏è  Im√°genes: {analysis_json.get('total_images', 0)}")

        # PASO 3: Cargar prompt LaTeX
        logger.info("\nüìù PASO 3/4: Cargando prompt LaTeX...")

        prompt_file_name = os.getenv('PROMPT_LATEX_FILE', 'prompt_latex.txt')
        # Obtener directorio api_service/ (7 niveles arriba desde analysis.py)
        # analysis.py -> routes -> facebook -> apify -> routes -> api -> app -> api_service
        current_file = Path(__file__)
        api_service_dir = current_file.parent.parent.parent.parent.parent.parent.parent
        prompt_file_path = api_service_dir / prompt_file_name

        logger.info(f"   üîç Buscando prompt en: {prompt_file_path}")

        if not prompt_file_path.exists():
            raise HTTPException(
                500,
                f"Archivo de prompt LaTeX no encontrado: {prompt_file_name}. "
                f"Ruta buscada: {prompt_file_path}"
            )

        latex_prompt = prompt_file_path.read_text(encoding='utf-8')
        logger.info(f"   ‚úÖ Prompt cargado: {prompt_file_name}")

        # PASO 4: Generar LaTeX con OpenAI
        logger.info("\nü§ñ PASO 4/4: Generando c√≥digo LaTeX con OpenAI...")

        # Limpiar y parsear el campo 'analysis' si est√° como string
        if 'analysis' in analysis_json and isinstance(analysis_json['analysis'], str):
            try:
                # Remover markdown code blocks
                analysis_str = analysis_json['analysis']
                if '```json' in analysis_str:
                    analysis_str = analysis_str.split(
                        '```json')[1].split('```')[0]
                elif '```' in analysis_str:
                    analysis_str = analysis_str.split('```')[1].split('```')[0]

                analysis_str = analysis_str.strip()
                parsed_analysis = json_module.loads(analysis_str)
                analysis_json['parsed_analysis'] = parsed_analysis
                logger.info("   ‚úÖ An√°lisis parseado correctamente")
            except Exception as e:
                logger.warning(
                    f"   ‚ö†Ô∏è No se pudo parsear 'analysis': {str(e)}")

        # Preparar mensaje: Prompt + JSON de an√°lisis
        message_content = f"{latex_prompt}\n\n"
        message_content += "**DATOS DE ENTRADA - An√°lisis en JSON:**\n\n"
        message_content += "```json\n"
        message_content += json_module.dumps(analysis_json,
                                             ensure_ascii=False, indent=2)
        message_content += "\n```\n\n"
        message_content += "Genera el c√≥digo LaTeX completo y profesional bas√°ndote en este an√°lisis."

        # Enviar a OpenAI
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "Eres un experto en LaTeX. Generas c√≥digo LaTeX profesional y compilable."
                },
                {
                    "role": "user",
                    "content": message_content
                }
            ],
            max_tokens=4096,
            temperature=0.2  # Muy baja para c√≥digo LaTeX preciso y compilable
        )

        latex_code = response.choices[0].message.content or ""
        tokens_used = response.usage.total_tokens if response.usage else 0

        logger.info(
            f"   ‚úÖ LaTeX generado ({tokens_used} tokens, {len(latex_code)} chars)")

        # Limpiar c√≥digo LaTeX (remover markdown si existe)
        if latex_code.startswith("```latex"):
            latex_code = latex_code.replace("```latex", "", 1)
        if latex_code.startswith("```"):
            latex_code = latex_code.replace("```", "", 1)
        if latex_code.endswith("```"):
            latex_code = latex_code[:-3]
        latex_code = latex_code.strip()

        # Guardar archivo .tex
        logger.info("   üíæ Guardando archivo .tex...")
        reports_dir.mkdir(parents=True, exist_ok=True)

        tex_filename = f"{run_id}_report.tex"
        tex_path = reports_dir / tex_filename

        with open(tex_path, 'w', encoding='utf-8') as f:
            f.write(latex_code)

        logger.info(f"   ‚úÖ Guardado: {tex_filename}")
        logger.info("\n" + "="*80)
        logger.info("üéâ C√ìDIGO LATEX GENERADO EXITOSAMENTE")
        logger.info("="*80)
        logger.info(f"üìÑ Archivo: {tex_path}")
        logger.info("\nüí° Para compilar a PDF:")
        logger.info(f"   cd {reports_dir}")
        logger.info(f"   pdflatex {tex_filename}")
        logger.info("="*80)

        return {
            'status': 'success',
            'run_id': run_id,
            'tex_file': str(tex_path),
            'tex_filename': tex_filename,
            'latex_code': latex_code,
            'tokens_used': tokens_used,
            'model': 'gpt-4o',
            'timestamp': datetime.now().isoformat(),
            'compile_instructions': {
                'windows_miktex': f'pdflatex {tex_filename}',
                'linux_texlive': f'pdflatex {tex_filename}',
                'online_overleaf': 'Subir archivo .tex a Overleaf y compilar'
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error("="*80)
        logger.error(f"‚ùå RUN_ID {run_id}: ERROR EN GENERACI√ìN LATEX")
        logger.error("="*80)
        logger.error(f"   üî¥ Tipo: {type(e).__name__}")
        logger.error(f"   üî¥ Mensaje: {str(e)}")
        logger.exception("   üìã Traceback:")
        raise HTTPException(500, f"Error: {str(e)}")


@router.post(
    '/compile-latex-to-pdf',
    tags=["ai-analysis"],
    summary="Compila archivo LaTeX a PDF"
)
async def compile_latex_endpoint(run_id: str) -> Dict[str, Any]:
    """
    Compila un archivo .tex existente a PDF usando pdflatex.

    Este endpoint:
    1. Busca el archivo .tex del run_id
    2. Ejecuta pdflatex para compilar a PDF
    3. Limpia archivos auxiliares (.aux, .log, etc)
    4. Guarda el PDF en el mismo directorio

    Requisitos:
    - pdflatex instalado (MiKTeX en Windows, TeX Live en Linux/Mac)

    Args:
        run_id: ID de la campa√±a con archivo .tex generado

    Returns:
        JSON con ruta del PDF y resultado de compilaci√≥n
    """
    import json as json_module

    logger.info("="*80)
    logger.info("üìÑ COMPILACI√ìN LATEX A PDF")
    logger.info("="*80)
    logger.info(f"üìã Run ID: {run_id}")

    try:
        # PASO 1: Validar directorio
        logger.info("\nüìÅ PASO 1/3: Validando directorios...")

        # Usar la funci√≥n helper para obtener la ruta base correcta
        base_dir = get_facebook_saved_base()
        reports_dir = base_dir / 'reports_json'

        if not reports_dir.exists():
            raise HTTPException(
                404,
                f"Directorio no encontrado: {reports_dir}. "
                f"Ejecuta primero /generate-latex-report"
            )

        logger.info(f"   ‚úÖ Directorio: {reports_dir}")

        # PASO 2: Buscar archivo .tex
        logger.info("\nüìÑ PASO 2/3: Buscando archivo .tex...")

        tex_filename = f"{run_id}_report.tex"
        tex_path = reports_dir / tex_filename

        if not tex_path.exists():
            raise HTTPException(
                404,
                f"Archivo .tex no encontrado: {tex_filename}. "
                f"Ejecuta primero /generate-latex-report"
            )

        logger.info(f"   ‚úÖ Archivo encontrado: {tex_filename}")
        logger.info(f"   üìä Tama√±o: {tex_path.stat().st_size} bytes")

        # PASO 3: Compilar a PDF
        logger.info("\nüî® PASO 3/3: Compilando LaTeX a PDF...")

        pdf_result = compile_latex_to_pdf(
            tex_path=tex_path,
            output_dir=reports_dir
        )

        if pdf_result['success']:
            pdf_filename = pdf_result['pdf_filename']
            pdf_path = reports_dir / pdf_filename

            logger.info(f"   ‚úÖ PDF generado: {pdf_filename}")
            logger.info(f"   üìä Tama√±o: {pdf_path.stat().st_size} bytes")
            logger.info("\n" + "="*80)
            logger.info("üéâ PDF GENERADO EXITOSAMENTE")
            logger.info("="*80)
            logger.info(f"üìÑ Archivo: {pdf_path}")

            return {
                'status': 'success',
                'run_id': run_id,
                'pdf_file': str(pdf_path),
                'pdf_filename': pdf_filename,
                'pdf_size_bytes': pdf_path.stat().st_size,
                'tex_file': str(tex_path),
                'timestamp': datetime.now().isoformat()
            }
        else:
            error_msg = pdf_result['error']
            logger.error("\n" + "="*80)
            logger.error("‚ùå ERROR EN COMPILACI√ìN PDF")
            logger.error("="*80)
            logger.error(f"   üî¥ Error: {error_msg}")

            raise HTTPException(
                500,
                f"Error al compilar PDF: {error_msg}. "
                f"Aseg√∫rate de tener pdflatex instalado (MiKTeX o TeX Live)"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error("="*80)
        logger.error(f"‚ùå RUN_ID {run_id}: ERROR EN COMPILACI√ìN")
        logger.error("="*80)
        logger.error(f"   üî¥ Tipo: {type(e).__name__}")
        logger.error(f"   üî¥ Mensaje: {str(e)}")
        logger.exception("   üìã Traceback:")
        raise HTTPException(500, f"Error: {str(e)}")


@router.post(
    '/generate-pdf-report',
    tags=["ai-analysis"],
    summary="Genera reporte PDF directamente desde an√°lisis JSON"
)
async def generate_pdf_report(run_id: str) -> Dict[str, Any]:
    """
    Genera PDF profesional directamente desde an√°lisis JSON usando ReportLab.

    NO requiere pdflatex instalado - generaci√≥n 100% Python.

    Este endpoint:
    1. Lee el archivo JSON de an√°lisis del run_id
    2. Usa ReportLab para generar PDF directamente
    3. Guarda el PDF en el mismo directorio
    4. Devuelve informaci√≥n del PDF generado

    Args:
        run_id: ID de la campa√±a con an√°lisis JSON guardado

    Returns:
        JSON con ruta del PDF y detalles
    """
    import json as json_module

    logger.info("="*80)
    logger.info("üìÑ GENERACI√ìN DE PDF CON REPORTLAB")
    logger.info("="*80)
    logger.info(f"üìã Run ID: {run_id}")

    try:
        # PASO 1: Buscar an√°lisis JSON
        logger.info("\nüìÇ PASO 1/2: Buscando an√°lisis JSON...")

        base_dir = get_facebook_saved_base()
        reports_dir = base_dir / 'reports_json'

        json_path = reports_dir / f"{run_id}_local_analysis.json"

        if not json_path.exists():
            raise HTTPException(
                404,
                f"No se encontr√≥ an√°lisis JSON para run_id: {run_id}. "
                f"Ejecuta primero /analyze-local-only"
            )

        logger.info(f"   ‚úÖ JSON encontrado: {json_path.name}")

        # Leer JSON
        with open(json_path, 'r', encoding='utf-8') as f:
            analysis_json = json_module.load(f)

        logger.info(
            f"   üìä Anuncios: {analysis_json.get('total_ads_analyzed', 0)}")

        # PASO 2: Generar PDF
        logger.info("\nüìÑ PASO 2/2: Generando PDF con ReportLab...")

        pdf_filename = f"{run_id}_report.pdf"
        pdf_path = reports_dir / pdf_filename

        result = create_pdf_from_analysis(
            analysis_json=analysis_json,
            output_path=pdf_path,
            run_id=run_id
        )

        if result['success']:
            pdf_size = pdf_path.stat().st_size

            logger.info(f"   ‚úÖ PDF generado: {pdf_filename}")
            logger.info(f"   üìä Tama√±o: {pdf_size:,} bytes")
            logger.info("\n" + "="*80)
            logger.info("üéâ PDF GENERADO EXITOSAMENTE")
            logger.info("="*80)
            logger.info(f"üìÑ Archivo: {pdf_path}")

            return {
                'status': 'success',
                'run_id': run_id,
                'pdf_file': str(pdf_path),
                'pdf_filename': pdf_filename,
                'pdf_size_bytes': pdf_size,
                'generator': 'reportlab',
                'timestamp': datetime.now().isoformat()
            }
        else:
            logger.error("\n" + "="*80)
            logger.error("‚ùå ERROR EN GENERACI√ìN PDF")
            logger.error("="*80)
            logger.error(f"   üî¥ Error: {result['error']}")

            raise HTTPException(
                500, f"Error al generar PDF: {result['error']}")

    except HTTPException:
        raise
    except Exception as e:
        logger.error("="*80)
        logger.error(f"‚ùå RUN_ID {run_id}: ERROR EN GENERACI√ìN PDF")
        logger.error("="*80)
        logger.error(f"   üî¥ Tipo: {type(e).__name__}")
        logger.error(f"   üî¥ Mensaje: {str(e)}")
        logger.exception("   üìã Traceback:")
        raise HTTPException(500, f"Error: {str(e)}")
